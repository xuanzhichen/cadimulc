{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>CADIMULC is a Python package standing for: CAusal DIscovery  with Multiple Latent Confounders, providing easy-to-use light APIs  to learn an empirical causal graph from generally raw data with relatively efficiency. It integrates implementations of hybrid-based approaches involving the popular MLC-LiNGAM algorithm, along with the \"micro\" workflow of causal discovery, such as data generation, learning results evaluation, and graphs visualization.</p>"},{"location":"#who-we-are","title":"Who We Are?","text":"<p>The hybrid methodology built in CADIMULC refers to the causal discovery framework that was early proposed and developed by the  Data Mining and Information Retrieval laboratory (DMIR lab, PI: Ruichu Cai). The lab has also been collaboratively developing causal-learn (Python package for causal discovery) by CMU.</p> <p>Xuanzhi Chen is currently the owner of the repository, but maintenance and updates might not be timely since CADIMULC is limited in personal development.  Xuanzhi Chen is sorry about that, but efforts of opening the issue and  advancing the causal discovery community are always welcomed. (Reach Out: xuanzhichen.42@gmail.com)</p>"},{"location":"#when-might-i-need-cadimulc","title":"When Might I Need CADIMULC?","text":"<p>Suppose you only have a dataset with respect to 10 variables you are interest in, you could use CADIMULC if you believe the causal relations with respect to the 10 variables  can be automatically discovered from the data itself.</p> <p>Aside for these 10 variables, are there exist additionally unobserved variables that associates with  the dataset in reality? Is the causal relation the linear relation, or the non-linear one?</p> <p>These are important questions before one begins. Yet CADIMULC is developed for deal with the concerns, so that users are able to start causal discovery straightforwardly.</p>"},{"location":"#overview","title":"Overview","text":""},{"location":"#what-is-the-hybrid-based-approach-in-causal-discovery","title":"What Is the Hybrid-Based Approach in Causal Discovery?","text":"<p>In fields of causal discovery,  conditional-independence-test (CIT) based methods and functional-causal-models (FCMs) based methods are the dominantly popular methodology over the last two decades.</p> <p>Representative hybrid-based causal discovery algorithms, such as  SADA [1], MLC-LiNGAM [2], and Nonlinear-MLC [3], are meant to hybridize the advantages of the two theory.</p> <p>The following picture shows the theory blueprint for the algorithmic programming:</p> <p>For details, please refer to the main idea  in CADIMULC's related paper, Section 4.</p> <ol> <li>resort to CIT approaches to construct network skeleton (i); </li> <li>apply FCMs approaches based on the well-known divide-and-conquer strategy (ii).</li> </ol>"},{"location":"#the-defining-feature-of-cadimulc","title":"The Defining Feature of Cadimulc","text":"<p>Equipped with the repertoire of hybrid-based approaches,  CADIMULC further focuses on the algorithmic efficiency and robustness in general causal discovery. Namely, to relatively relax causal assumptions, such as causal sufficiency and linearity, upon the data. For example:</p> <ul> <li>given common disturbance from the multiple unknown factor;</li> <li>given the \"generic\" non-linear relation entailed by data.</li> </ul> <p>Thus, causal discovery by CADIMULC is still anticipated to be  relatively scalable in computation and reliable in performance under these challenges.</p>"},{"location":"#main-modules-for-the-micro-workflow-of-causal-discovery","title":"Main Modules for the \"Micro\" Workflow of Causal Discovery","text":"Module Workflow Description hybrid_algorithms.MLCLiNGAM Causal Discovery The hybrid-based causal discovery algorithm adaptive to linear data relations hybrid_algorithms.NonlinearMLC Causal Discovery The hybrid-based causal discovery algorithm adaptive to non-linear data relations utils.Generator Data generation Simulation for Structural Causal Models (SCMs) utils.Evaluator Evaluation Causal graph evaluation akin to the common evaluation used in machine learning"},{"location":"#citation","title":"Citation","text":"<ul> <li> <p>Please cite the following paper if you would like to use the MLC-LiNGAM approach in your reports or publications: <pre><code>@article{chen2021causal,\n  title={Causal discovery in linear non-gaussian acyclic model with multiple latent confounders},\n  author={Chen, Wei and Cai, Ruichu and Zhang, Kun and Hao, Zhifeng},\n  journal={IEEE Transactions on Neural Networks and Learning Systems},\n  volume={33},\n  number={7},\n  pages={2816--2827},\n  year={2021},\n  publisher={IEEE}\n}\n</code></pre></p> </li> <li> <p>The paper for the Nonlinear-MLC approach is unpublished.  You could choose to support it on the CADIMULC github repository. </p> </li> </ul>"},{"location":"#license","title":"License","text":"<p>Copyright (C) 2022-2024 Xuanzhi Chen (DMIR lab, Guangdong University of Technology, China)</p> <p>CADIMULC is downloaded for free, provided \"as is\" WITHOUT ANY EXPRESS OR IMPLIED WARRANTY; CADIMULC is developed in hope of being beneficial for empirical data analysis in causation, but WITHOUT WARRANTY OF ABSOLUTELY ACCURATE INTERPRETATION.</p>"},{"location":"#reference","title":"Reference","text":"<p>[1] Cai, Ruichu, Zhenjie Zhang, and Zhifeng Hao.  Sada: A general framework to support robust causation discovery. International conference on machine learning, PMLR. 2013.</p> <p>[2] Chen, Wei, Ruichu Cai, Kun Zhang, and Zhifeng Hao. \"Causal discovery in linear non-gaussian acyclic model with multiple latent confounders. \" IEEE Transactions on Neural Networks and Learning Systems. 2021.</p> <p>[3] Chen, Xuanzhi, Wei Chen, Ruichu Cai.  \"Non-linear Causal Discovery for Additive Noise Model with     Multiple Latent Confounders\". Xuanzhi's Personal Website. 2023.</p>"},{"location":"acknowledgements/","title":"Acknowledgements","text":"<p>Xuanzhi Chen would like to thank the DMIR laboratory for offering him research opportunities,  with special thanks to Ruichu Cai (\u8521\u745e\u521d) of the lab director and Dongning Liu (\u5218\u51ac\u5b81) of the dean in School of Computer.</p> <p>Jie Qiao (\u4e54\u6770) and Zhiyi Huang (\u9ec4\u667a\u6bc5) were willing to spend their time in personal discussions with Xuanzhi Chen  about details in the paper; Zhengming Chen (\u9648\u6b63\u94ed) patiently helped point out initial mistakes in the paper; thanks to other graduate students, such as Zeqin Yang (\u6768\u6cfd\u52e4), Zhengting Huang (\u9ec4\u6b63\u5a77), Xiaokai Huang (\u9ec4\u6653\u6977), and Yu Xiang (\u5411\u5b87), for their generosity of teaching when Xuanzhi chen was initially building the CADIMULC repository.</p> <p>Finally, Xuanzhi Chen owes a great debt to his advisor Wei Chen (\u9648\u8587) for her encouragement  when Xuanzhi started studying causation two years ago \u2014 \"Do it, just have your own interest of research and your own rhythm of lifetime\".</p>"},{"location":"evaluation/","title":"Evaluation and Visualization","text":"<p>Advice for developers if needed: Evaluation and visualization</p> <p>Algorithms in CADIMULC simply represent the causation among variables, for both ground-truth  and learning results, as the directed pairs in an adjacency matrix with only two elements 0 and 1.</p> <p>If you incline to this representation of data structure in your work or research,  then <code>Evaluator</code> in CADIMULC might provide you convenience for evaluating the causal graph directly.</p> Class: Evaluator <p>---</p> Primary Method: precision_pairwise <p>Causal pair precision refers to the proportion of the correctly estimated directed pairs in all the estimated directed pairs (EDP):</p> \\[     Precision = TP \\ / \\  (TP + FP) = TP \\ / \\ EDP. \\] <p>The higher the precision, the larger the amount of the causal pairs, compared to EDP, that are identified, without considering the amount of unestimated pairs.</p> <p>Parameters:</p> Name Type Description Default <code>true_graph</code> <code>ndarray</code> <p>True causal graph, namely the ground-truth.</p> required <code>est_graph</code> <code>ndarray</code> <p>Estimated causal graph, namely the empirical causal graph.</p> required <p>Returns:</p> Name Type Description <code>precision</code> <code>float</code> <p>Precision of the \"causal discovery task\".</p> Source code in <code>cadimulc\\utils\\evaluation.py</code> <pre><code>@staticmethod\ndef precision_pairwise(true_graph: ndarray, est_graph: ndarray) -&gt; float:\n    \"\"\"\n    **Causal pair precision** refers to the proportion of the correctly estimated\n    directed pairs in all the **estimated directed pairs** (EDP):\n\n    $$\n        Precision = TP \\ / \\  (TP + FP) = TP \\ / \\ EDP.\n    $$\n\n    The higher the precision, the larger the amount of the causal pairs,\n    compared to EDP, that are identified,\n    without considering the amount of **unestimated** pairs.\n\n    Parameters:\n        true_graph: True causal graph, namely the ground-truth.\n        est_graph: Estimated causal graph, namely the empirical causal graph.\n        learned from data.\n\n    Returns:\n        precision: Precision of the \"causal discovery task\".\n    \"\"\"\n\n    _, dict_directed_parent = Evaluator.get_pairwise_info(true_graph)\n    est_pairs = Evaluator.get_directed_pairs(est_graph)\n    num_est_pairs = len(est_pairs)\n\n    if num_est_pairs &gt; 0:\n        tp = 0\n\n        for est_pair in est_pairs:\n            child = est_pair[1]\n            parent = est_pair[0]\n\n            if parent in dict_directed_parent[child]:\n                tp += 1\n\n        precision = round((tp / num_est_pairs), 3)\n\n    else:\n        precision = float(0)\n\n    return precision\n</code></pre> Primary Method: recall_pairwise <p>Causal pair recall refers to the proportion of correctly estimated directed pairs in all true causal pairs (TCP):</p> \\[     Recall = TP \\ / \\  (TP + FN) = TP \\ / \\  TCP \\] <p>The higher the recall, the larger the amount of the causal pairs, compared to TCP, that are identified, without considering the amount of incorrectly estimated pairs.</p> <p>Parameters:</p> Name Type Description Default <code>true_graph</code> <code>ndarray</code> <p>True causal graph, namely the ground-truth.</p> required <code>est_graph</code> <code>ndarray</code> <p>Estimated causal graph, namely the empirical causal graph.</p> required <p>Returns:</p> Name Type Description <code>recall</code> <code>float</code> <p>Recall of the \"causal discovery task\".</p> Source code in <code>cadimulc\\utils\\evaluation.py</code> <pre><code>@staticmethod\ndef recall_pairwise(true_graph: ndarray, est_graph: ndarray) -&gt; float:\n    \"\"\"\n    **Causal pair recall** refers to the proportion of correctly estimated directed\n    pairs in all **true causal pairs** (TCP):\n\n    $$\n        Recall = TP \\ / \\  (TP + FN) = TP \\ / \\  TCP\n    $$\n\n    The higher the recall, the larger the amount of the causal pairs,\n    compared to TCP,\n    that are identified, without considering the amount of **incorrectly** estimated pairs.\n\n    Parameters:\n        true_graph: True causal graph, namely the ground-truth.\n        est_graph: Estimated causal graph, namely the empirical causal graph.\n\n    Returns:\n        recall: Recall of the \"causal discovery task\".\n    \"\"\"\n\n    num_directed_pairs, dict_directed_parent = (\n        Evaluator.get_pairwise_info(true_graph))\n\n    est_pairs = Evaluator.get_directed_pairs(est_graph)\n    num_est_pairs = len(est_pairs)\n\n    if num_directed_pairs == 0:\n        recall = float(1)\n\n    elif num_est_pairs == 0:\n        recall = float(0)\n\n    else:\n        tp = 0\n\n        for est_pair in est_pairs:\n            child = est_pair[1]\n            parent = est_pair[0]\n\n            if parent in dict_directed_parent[child]:\n                tp += 1\n\n        recall = round((tp / num_directed_pairs), 3)\n\n    return recall\n</code></pre> Primary Method: f1_score_pairwise <p>Causal pair F1-score, the concordant mean of the precision and recall, represents the global measurement of causal discovery, bring together the advantages from both the precision and recall.</p> \\[     F1 = (2 * Precision * Recall)\\  / \\ (Precision + Recall). \\] <p>Parameters:</p> Name Type Description Default <code>true_graph</code> <code>ndarray</code> <p>True causal graph, namely the ground-truth.</p> required <code>est_graph</code> <code>ndarray</code> <p>Estimated causal graph, namely the empirical causal graph.</p> required <p>Returns:</p> Name Type Description <code>f1_score</code> <code>float</code> <p>F1-score of the \"causal discovery task\".</p> Source code in <code>cadimulc\\utils\\evaluation.py</code> <pre><code>@staticmethod\ndef f1_score_pairwise(true_graph: ndarray, est_graph: ndarray) -&gt; float:\n    \"\"\"\n    **Causal pair F1-score**, the concordant mean of the precision and recall,\n    represents the global measurement of causal discovery, bring together the\n    advantages from both the precision and recall.\n\n    $$\n        F1 = (2 * Precision * Recall)\\  / \\ (Precision + Recall).\n    $$\n\n    Parameters:\n        true_graph: True causal graph, namely the ground-truth.\n        est_graph: Estimated causal graph, namely the empirical causal graph.\n\n    Returns:\n        f1_score: F1-score of the \"causal discovery task\".\n    \"\"\"\n\n    precision = Evaluator.precision_pairwise(true_graph, est_graph)\n    recall = Evaluator.recall_pairwise(true_graph, est_graph)\n\n    if (precision + recall) != 0:\n        f1_score = round(\n            (2 * precision * recall) / (precision + recall), 3\n        )\n\n    else:\n        f1_score = float(0)\n\n    return f1_score\n</code></pre> Primary Method: evaluate_skeleton <p>Note</p> <p>Construction of a network skeleton is the fundamental part relative to the procedure of hybrid-based approaches.  CADIMULC also provides simply way to evaluate the causal skeleton. Notice that performance of the hybrid-based approach largely depends on the initial performance of the causal skeleton learning.</p> <p>The <code>evaluate_skeleton</code> method evaluates a network skeleton based on an assigned metric. To this end, available metrics mirroring to the causal pair evaluation  are list as the following:</p> <ul> <li>Skeleton Precision = TP (of the estimated skeleton) / all estimated edges.</li> <li>Skeleton Recall = TP (of the estimated skeleton) / all true edges.</li> <li>Skeleton F1-score = (2 * Precision * Recall) / (Precision + Recall).</li> </ul> <p>Parameters:</p> Name Type Description Default <code>true_skeleton</code> <code>ndarray</code> <p>True causal skeleton, namely the ground-truth.</p> required <code>est_skeleton</code> <code>ndarray</code> <p>Estimated causal skeleton, namely the empirical causal skeleton.</p> required <code>metric</code> <code>str</code> <p>selective metrics from <code>['Precision', 'Recall', or 'F1-score']</code>.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The evaluating value of the causal skeleton in light of the assigned metric.</p> Source code in <code>cadimulc\\utils\\evaluation.py</code> <pre><code>@staticmethod\ndef evaluate_skeleton(\n        true_skeleton: ndarray,\n        est_skeleton: ndarray,\n        metric: str\n) -&gt; float:\n    \"\"\"\n    The `evaluate_skeleton` method evaluates a network skeleton based on an assigned\n    metric. To this end, available metrics mirroring to the causal pair evaluation\n     are list as the following:\n\n    * **Skeleton Precision** = TP (of the estimated skeleton) / all estimated edges.\n    * **Skeleton Recall** = TP (of the estimated skeleton) / all true edges.\n    * **Skeleton F1-score** = (2 * Precision * Recall) / (Precision + Recall).\n\n    Parameters:\n        true_skeleton: True causal skeleton, namely the ground-truth.\n        est_skeleton: Estimated causal skeleton, namely the empirical causal skeleton.\n        metric: selective metrics from `['Precision', 'Recall', or 'F1-score']`.\n\n    Returns:\n        The evaluating value of the causal skeleton in light of the assigned metric.\n    \"\"\"\n\n    true_skeleton_nx = nx.from_numpy_array(true_skeleton)\n    est_skeleton_nx = nx.from_numpy_array(est_skeleton)\n\n    true_edges = list(true_skeleton_nx.edges())\n    est_edges = list(est_skeleton_nx.edges())\n\n    tp_skeleton = 0\n    for est_edge in est_edges:\n        if est_edge in true_edges:\n            tp_skeleton += 1\n\n    if len(est_edges) &gt; 0:\n        precision = round(tp_skeleton / len(est_edges), 3)\n    else:\n        precision = float(0)\n\n    if len(true_edges) &gt; 0:\n        recall = round(tp_skeleton / len(true_edges), 3)\n    else:\n        recall = float(0)\n\n    if precision + recall &gt; 0:\n        f1_score = round(\n            2 * ((precision * recall) / (precision + recall)), 3\n        )\n    else:\n        f1_score = float(0)\n\n    if metric == 'Precision':\n        return precision\n\n    elif metric == 'Recall':\n        return recall\n\n    elif metric == 'F1-score':\n        return f1_score\n\n    else:\n        raise ValueError(\"Please input established metric types: \"\n                         \"'Precision', 'Recall', or 'F1-score'.\")\n</code></pre> Secondary Method: get_directed_pairs <p>Extract directed pairs from a graph.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>ndarray</code> <p>An adjacency bool matrix representing the causation among variables.</p> required <p>Returns:</p> Name Type Description <code>direct_pairs</code> <code>list[list]</code> <p>A list whose elements are in form of [parent, child], referring to the causation parent -&gt; child.</p> Source code in <code>cadimulc\\utils\\evaluation.py</code> <pre><code>@staticmethod\ndef get_directed_pairs(graph: ndarray) -&gt; list[list]:\n    \"\"\"\n    Extract directed pairs from a graph.\n\n    Parameters:\n        graph: An adjacency bool matrix representing the causation among variables.\n\n    Returns:\n        direct_pairs:\n            A list whose elements are in form of [parent, child], referring to the\n            causation parent -&gt; child.\n    \"\"\"\n\n    dim = graph.shape[0]\n    directed_pairs = []\n\n    for j in range(dim):\n        for i in range(dim):\n            if graph[i][j] == 1 and graph[j][i] == 0:\n                directed_pairs.append([j, i])\n\n    return directed_pairs\n</code></pre> Secondary Method: get_pairwise_info <p>Obtain information related to a given directed graph: (1) number of the directed pairs; (2) parents-child pairing relationships.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>ndarray</code> <p>An adjacency bool matrix representing the causation among variables.</p> required <p>Returns:</p> Type Description <code>(int, dict)</code> <p><code>num_directed_pairs</code> as the number of directed pairs and <code>directed_parent_dict</code> as the dictionary representing the parent-child pairing relationships.</p> Source code in <code>cadimulc\\utils\\evaluation.py</code> <pre><code>@staticmethod\ndef get_pairwise_info(graph: ndarray) -&gt; (int, dict):\n    \"\"\"\n    Obtain information related to a given directed graph:\n    (1) number of the directed pairs; (2) parents-child pairing relationships.\n\n    Parameters:\n        graph: An adjacency bool matrix representing the causation among variables.\n\n    Returns:\n        `num_directed_pairs` as the number of directed pairs and `directed_parent_dict`\n         as the dictionary representing the parent-child pairing relationships.\n    \"\"\"\n\n    directed_parent_dict = {}\n    num_directed_pairs = 0\n    dim = graph.shape[0]\n\n    for i in range(dim):\n        directed_parent_dict[i] = set()\n\n    for i in range(dim):\n        for j in range(dim):\n            if graph[i][j] == 1:\n                directed_parent_dict[i].add(j)\n                num_directed_pairs += 1\n\n    return num_directed_pairs, directed_parent_dict\n</code></pre> Function: draw_graph_from_ndarray <p>Running examples</p> <p>CADIMULC is a light Python repository without sophisticated library API design. Documentation on this page is meant to provide introductory materials of the practical tool as to causal discovery. For running example,  please simply check out Quick Tutorials for the straightforward usage in the \"micro\" workflow of  causal discovery.</p>"},{"location":"evaluation/#cadimulc.utils.evaluation.Evaluator","title":"<code>cadimulc.utils.evaluation.Evaluator</code>","text":"<p>Given an instance as to causal discovery, the <code>Evaluator</code> defines the classification errors between an actual graph and a predicted graph, which is corresponding to, in the field of machine learning, the four of the categories within a confusion matrix.</p> <ul> <li> <p>TP (True Positives): The number of the estimated directed pairs that   are consistent with the true causal pairs. Namely, TP qualifies the   correct estimation of causal relations.</p> </li> <li> <p>FP (False Positives): The number of the estimated directed pairs that   do not present in the true causal pairs.</p> </li> <li> <p>TN: (True Negatives): The number of the unestimated directed pairs   that are consistent with the true causal pairs. TN reflects   the correct prediction of unpresented causal relations.</p> </li> <li> <p>FN (False Negatives): The number of the unestimated directed pairs   that do present in the true causal pairs.</p> </li> </ul> <p>The only assessment of directed causal relations</p> <p>The <code>Evaluator</code> focuses on the assessment of estimated directed pairs (TP) extracted from an adjacency matrix, treating the rest as unpresented pairs (FP) relative to the ground-truth.</p> <p>In other words, <code>Evaluator</code> in CADIMULC does not explicitly consider bi-directed pairs or undirected pairs.</p>"},{"location":"evaluation/#cadimulc.utils.visualization.draw_graph_from_ndarray","title":"<code>cadimulc.utils.visualization.draw_graph_from_ndarray(array, graph_type='auto', rename_nodes=None, testing_text=None, save_fig=False, saving_path=None)</code>","text":"<p>Draw the directed or undirected (causal) graph that is in form of adjacency matrix (implementation based on NetworkX).</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>the causal graph (directed) or causal skeleton (indirected) in form of adjacency matrix.</p> required <code>graph_type</code> <code>str</code> <p>use <code>directed</code> to forcedly plot a directed graph.</p> <code>'auto'</code> <code>rename_nodes</code> <code>list | None</code> <p>Rename the nodes consisting with the column of dataset (n * d).</p> <code>None</code> <code>testing_text</code> <code>str | None</code> <p>Add simple text to the figure.</p> <code>None</code> <code>save_fig</code> <code>bool</code> <p>Specify saving a figure or not. Make sure to enter the saving path if you specify <code>save_fig=True</code>.</p> <code>False</code> <code>saving_path</code> <code>str | None</code> <p>The image saving path along with your image file name. e.g. ../file_location/image_file_name.</p> <code>None</code> Source code in <code>cadimulc\\utils\\visualization.py</code> <pre><code>def draw_graph_from_ndarray(\n    array: ndarray,\n    graph_type: str = \"auto\",\n    rename_nodes: list | None = None,\n    testing_text: str | None = None,\n    save_fig: bool = False,\n    saving_path: str | None = None\n):\n    \"\"\"\n    Draw the directed or undirected (causal) graph that is in form of adjacency matrix\n    (implementation based on NetworkX).\n\n    Parameters:\n        array: the causal graph (directed) or causal skeleton (indirected) in form of adjacency matrix.\n        graph_type: use `directed` to forcedly plot a directed graph.\n        rename_nodes: Rename the nodes consisting with the column of dataset (n * d).\n        Default as \"X1,...Xd\".\n        testing_text: Add simple text to the figure.\n        save_fig: Specify saving a figure or not. Make sure to enter the saving path if you specify `save_fig=True`.\n        saving_path: The image saving path along with your image file name. e.g. ../file_location/image_file_name.\n    \"\"\"\n\n    # ensure / convert into nx.graph\n    if not isinstance(array, np.ndarray):\n        raise TypeError(\"The expected type of input object should be numpy array.\")\n\n    if graph_type == 'auto':\n        if (array == array.T).all():\n            directed = False\n        else:\n            directed = True\n    elif graph_type == 'directed':\n        directed = True\n    else:\n        raise ValueError(\"Choose the graph type as 'auto' or 'directed'.\")\n\n    # directed graph\n    if directed:\n        # conventional causal direction / transpose of matrix\n        G = nx.from_numpy_array(array.T, create_using=nx.DiGraph)\n    # undirected graph\n    else:\n        # conventional causal direction / transpose of matrix\n        G = nx.from_numpy_array(array.T)\n\n    # rename graph node name\n    if rename_nodes is None:\n        node_id = 0\n        for node in G.nodes():\n            rename_node = f'X{node_id + 1}'\n            G = nx.relabel_nodes(G, mapping={node: rename_node})\n            node_id += 1\n    else:\n        for node, rename_node in zip(G.nodes(), rename_nodes):\n            rename_node = str(rename_node)\n            G = nx.relabel_nodes(G, mapping={node: rename_node})\n\n    # fix position\n    pos = nx.circular_layout(G)\n    # setting parameters\n    plt.figure(figsize=(5, 2.7), dpi=120)\n\n    if directed:\n        nx.draw(\n            G=G,\n            pos=pos,\n            with_labels=True,\n            node_color='black',\n            font_color='white',\n            font_size=15,\n            width=1.25,\n            arrowsize=20,\n            node_size=500\n        )\n\n    else:\n        nx.draw(\n            G=G,\n            pos=pos,\n            with_labels=True,\n            node_color='black',\n            font_color='white',\n            font_size=15,\n            width=1.25,\n            node_size=500\n        )\n\n    if testing_text is not None:\n        plt.text(x=0.5, y=0.5, s=testing_text, fontsize=12, color='red')\n        print('* Figure Label: ', testing_text)\n\n    if save_fig:\n        plt.savefig(\n            saving_path + \".png\",\n            dpi=200,\n            transparent=False,\n            bbox_inches=\"tight\"\n        )\n</code></pre>"},{"location":"generation/","title":"SCMs Data Generation","text":"<p>Advice for developers if needed: SCMs data generation</p> <p><code>Generator</code> in CADIMULC serves as a framework of general data generation in  the task of causal discovery.  Default settings of hyperparameters (e.g. parameters of specific causal function)  in the <code>Generator</code> might require being fine-tuned depends on different purposes for simulation.</p> <p>Users could develop their own \"causal simulator\" in data analysis based on their need  interest, by following the data generation template in <code>Generator</code>.</p> Class: Generator Primary Method: run_generation_procedure <p>Run the common two-steps procedure for SCMs data generation:</p> <ol> <li>Generate a random DAG in light of the well-known Erd\u0151s\u2013R\u00e9nyi model;</li> <li>Provided a topological order converted by the DAG, generate each variable \\(y_i\\) by summarizing the effects of its parents \\(pa(y_i)\\).</li> </ol> <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Update the <code>Generator</code>'s attributes: <code>_skeleton</code> as the undirected graph corresponding to the causal graph (ground-truth), <code>_dag</code> as the directed acyclic graph (DAG) corresponding to the (ground-truth), and <code>_dataset</code> in a format (n * d) (n = <code>sample</code>, d = <code>graph_node_num</code>).</p> Source code in <code>cadimulc\\utils\\generation.py</code> <pre><code>def run_generation_procedure(self) -&gt; object:\n    \"\"\" Run the common **two-steps** procedure for SCMs data generation:\n\n    1. Generate a random DAG in light of the well-known Erd\u0151s\u2013R\u00e9nyi model;\n    2. Provided a topological order converted by the DAG, generate each variable $y_i$ by\n    summarizing the effects of its parents $pa(y_i)$.\n\n    Returns:\n        self:\n            Update the `Generator`'s attributes: `_skeleton` as the undirected graph\n            corresponding to the causal graph (ground-truth), `_dag` as the\n            directed acyclic graph (DAG) corresponding to the (ground-truth),\n            and `_dataset` in a format (n * d) (n = `sample`, d = `graph_node_num`).\n    \"\"\"\n\n    self._clear()\n\n    # Generate a random DAG in light of the well-known Erd\u0151s\u2013R\u00e9nyi model.\n    self._generate_dag()\n\n    # Provided a topological order converted by the DAG, generate each variable by\n    # summarizing the effects of its parents\n    self._generate_data()\n\n    return self\n</code></pre> Private Method: _generate_dag <p>Generate a random DAG in light of the well-known Erd\u0151s\u2013R\u00e9nyi model.</p> <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Update <code>_dag</code> (DAG) represented as a bool adjacency matrix.</p> Source code in <code>cadimulc\\utils\\generation.py</code> <pre><code>def _generate_dag(self) -&gt; object:\n    \"\"\"\n    Generate a random DAG in light of the well-known Erd\u0151s\u2013R\u00e9nyi model.\n\n    Returns:\n        self: Update `_dag` (DAG) represented as a bool adjacency matrix.\n    \"\"\"\n\n    # undirected graph\n    undigraph = self._get_undigraph(\n        graph_node_num=self.graph_node_num,\n        sparsity=self.sparsity\n    )\n    self.skeleton = copy(undigraph)\n\n    # directed graph\n    digraph = self._orient(undigraph)\n    self.dag = copy(digraph)\n\n    return self\n</code></pre> Private Method: _generate_data <p>Provided a topological order converted by the DAG, generate each variable by summarizing the effects of its parents.</p> <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Update <code>_data</code> represented as a (n * d) numpy array (n = sample, d = graph_node_num).</p> Source code in <code>cadimulc\\utils\\generation.py</code> <pre><code>def _generate_data(self) -&gt; object:\n    \"\"\"\n    Provided a topological order converted by the DAG, generate each variable by\n    summarizing the effects of its parents.\n\n    Returns:\n        self: Update `_data` represented as a (n * d) numpy array (n = sample,\n         d = graph_node_num).\n    \"\"\"\n\n    self.data = np.zeros([self.sample, self.graph_node_num])\n\n    # topological order converted by the DAG\n    dag_nx = nx.DiGraph(self.dag.T)\n    topo_order = list(nx.topological_sort(dag_nx))\n\n    # data generation in light of additive noise model\n    for child_index, child_var in enumerate(topo_order):\n        parent_vars = list(dag_nx.predecessors(child_var))\n        parent_indexes = [topo_order.index(var) for var in parent_vars]\n\n        # independence noise\n        self._add_random_noise(var_index=child_index)\n\n        # summary of the parents effects\n        if len(parent_vars) &gt; 0:\n            for parent_index in parent_indexes:\n                self._simulate_causal_model(\n                    child_index=child_index,\n                    parent_index=parent_index\n                )\n\n    # standard deviation of the dataset (default setting)\n    self.data = self.data / np.std(self.data, axis=0)\n\n    return self\n</code></pre> <p>Running examples</p> <p>CADIMULC is a light Python repository without sophisticated library API design. Documentation on this page is meant to provide introductory materials of the practical tool as to causal discovery. For running example,  please simply check out Quick Tutorials for the straightforward usage in the \"micro\" workflow of  causal discovery.</p>"},{"location":"generation/#cadimulc.utils.generation.Generator","title":"<code>cadimulc.utils.generation.Generator</code>","text":"<p>             Bases: <code>object</code></p> <p>The <code>Generator</code> simulates the empirical data implied by the structure causal models (SCMs). Primary parameters for <code>Generator</code>'s simulation consist of the model classes (e.g. linear or non-linear) and the (independent) noise distributions (e.g. Gaussian or non-Gaussian).</p> <p>Take causation in graphical context, where a variable \\(y_i\\) is supposed to be the effect of its parents \\(pa(y_i)\\). Then the data relative to \\(y_i\\) is expected to be generated given the (group of) data relative to \\(pa(y_i)\\), following the causal mapping mechanism \\(F\\) characterized as SCMs.</p> <p>Currently, research in causal discovery has suggested that structural-identifiable empirical data should be further generated by a special \"genus\" of the SCMs, which is normally referred to as the additive noise models (ANMs) shown in the following</p> \\[ y_{i} := F(pa(y_i), e_{i}):= \\sum_{x_{j} \\in pa(y_i)} f( x_{j}) + e_{i}, \\] <p>where \\(f(\\cdot)\\) denotes the linear or non-linear function, and \\(e_{i}\\) refers to the independent noise obeying the Gaussian or non-Gaussian distributions.</p> <p>Structural-identifiable SCMs simulation in CADIMULC in light of related literature</p> <p>linear: linear non-Gaussian acyclic models (LiNGAM)<sup>[1]</sup>, referring to the experiment setup by MLC-LiNGAM<sup>[2]</sup>.</p> <p>non-linear: causal additive models (CAM)<sup>[3]</sup>, referring to the experiment setup by CAM-UV<sup>[4]</sup>.</p>"},{"location":"generation/#cadimulc.utils.generation.Generator.__init__","title":"<code>__init__(graph_node_num, sample, causal_model='hybrid_nonlinear', noise_type='Gaussian', noise_scale='default', sparsity=0.3, _skeleton=None, _dag=None, _dataset=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>graph_node_num</code> <code>int</code> <p>Number of the vertex in a causal graph (ground-truth), which represents the number of the variable given a causal model (recommend: &lt; 15).</p> required <code>sample</code> <code>int</code> <p>Size of the dataset generated from the SCMs (recommend: &lt; 10000).</p> required <code>causal_model</code> <code>str</code> <p>Refer to structural-identifiable SCMs simulation in light of related literature. e.g. LiNGAM (str: lingam), CAM (str: hybrid_nonlinear).</p> <code>'hybrid_nonlinear'</code> <code>noise_type</code> <code>str</code> <p>Refer to structural-identifiable SCMs simulation in light of related literature. e.g. Gaussian (str: Gaussian), uniform distribution as non-Gaussian (str: non-Gaussian).</p> <code>'Gaussian'</code> <code>noise_scale</code> <code>int | str</code> <p>\"Default\" as following the experiment setup in light of related literature.</p> <code>'default'</code> <code>sparsity</code> <code>float</code> <p>Control the sparsity of a causal graph (ground-truth) (recommend: 0.3).</p> <code>0.3</code> <p>The causal model should be carefully paired with the noise type</p> <ul> <li>If <code>causal model = \"lingam\"</code>, the noise distribution     must satisfy <code>noise_type=\"non-Gaussian\"</code>.</li> <li>If <code>causal model = \"hybrid_nonlinear\"</code>, the noise distribution     can either choose <code>noise_type=\"Gaussian\"</code> or <code>noise_type=\"non-Gaussian\"</code>.     However, evaluation in CADIMULC suggests that Gaussian noise is more preferable     to yield identifiable results.</li> </ul>"},{"location":"generation/#reference","title":"Reference","text":"<p>[1] Shimizu, Shohei, Patrik O. Hoyer, Aapo Hyv\u00e4rinen, Antti Kerminen, and Michael Jordan.  \"A linear non-Gaussian acyclic model for causal discovery.\"  Journal of Machine Learning Research. 2006.</p> <p>[2] Chen, Wei, Ruichu Cai, Kun Zhang, and Zhifeng Hao. \"Causal discovery in linear non-gaussian acyclic model with multiple latent confounders. \" IEEE Transactions on Neural Networks and Learning Systems. 2021.</p> <p>[3] B\u00fchlmann, Peter, Jonas Peters, and Jan Ernest.  \"CAM: Causal additive models, high-dimensional order search and penalized regression.\"  2014. </p> <p>[4] Maeda, Takashi Nicholas, and Shohei Shimizu.  \"Causal additive models with unobserved variables.\"  In Uncertainty in Artificial Intelligence. 2021.</p>"},{"location":"hybrid_algorithms/","title":"Hybrid-Based Approaches","text":"<p>Running examples</p> <p>CADIMULC is a light Python repository without sophisticated library API design. Documentation on this page is meant to provide introductory materials of the practical tool as to causal discovery. For running example,  please simply check out Quick Tutorials for the straightforward usage in the \"micro\" workflow of  causal discovery.</p> Class: MLC-LiNGAM <sup>[1]</sup> <p>The output corresponding to the MLC-LiNGAM algorithm</p> <p>In the field of causal discovery, causal graphs are usually represented as the directed acyclic graph (DAG) or the causal order.  The existence of latent confounders, however, might result in the causal relations that cannot be determined by algorithms. </p> <p>Correspondingly, the estimated causal graph, by the MLC-LiNGAM or the Nonlinear-MLC algorithm in CADIMULC, is represented as the partial directed acyclic graph or  the partial causal order.</p> Primary Method: fit <p>Fitting data via the MLC-LiNGAM causal discovery algorithm:</p> <ul> <li>Stage-I: Utilize the constraint-based method to learn a causal skeleton.</li> <li>Stage-II: Identify the causal directions by conducting regression and independence tests on the adjacent pairs in the causal skeleton.</li> <li>Stage-III: Detect the latent confounders with the help of the maximal clique patterns raised by the latent confounders, and uncover the causal structure with latent variables.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>ndarray</code> <p>The observational dataset shown as a matrix or table, with a format of \"sample (n) * dimension (d).\" (input as Pandas dataframe is also acceptable)</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Update the <code>adjacency_matrix</code> represented as an estimated causal graph. The <code>adjacency_matrix</code> is a (d * d) numpy array with 0/1 elements characterizing the causal direction.</p> Source code in <code>cadimulc\\hybrid_algorithms\\hybrid_algorithms.py</code> <pre><code>def fit(self, dataset: ndarray) -&gt; object:\n    \"\"\"\n    Fitting data via the *MLC-LiNGAM* causal discovery algorithm:\n\n    - **Stage-I**: Utilize the constraint-based method to learn a **causal skeleton**.\n    - **Stage-II**: Identify the causal directions by conducting **regression** and **independence tests**\n    on the adjacent pairs in the causal skeleton.\n    - **Stage-III**: Detect the latent confounders with the help of the **maximal clique patterns**\n    raised by the latent confounders,\n    and uncover the causal structure with latent variables.\n\n    Parameters:\n        dataset:\n            The observational dataset shown as a matrix or table,\n            with a format of \"sample (n) * dimension (d).\"\n            (input as Pandas dataframe is also acceptable)\n\n    Returns:\n        self:\n            Update the ``adjacency_matrix`` represented as an estimated causal graph.\n            The ``adjacency_matrix`` is a (d * d) numpy array with 0/1 elements\n            characterizing the causal direction.\n    \"\"\"\n\n    # stage-1: causal skeleton reconstruction(PC-stable algorithm)\n    self._stage_1_learning(dataset)\n\n    graph_pattern_manager = GraphPatternManager(init_graph=self._skeleton)\n\n    # stage-2: partial causal orders identification\n    self._stage_2_learning(graph_pattern_manager)\n\n    graph_pattern_manager.store_last_managing_adjacency_matrix()\n\n    # stage-3: latent confounders' detection\n    self._stage_3_learning(graph_pattern_manager)\n\n    return self\n</code></pre> Private Method: _stage_1_learning <p>Stage-I: Causal skeleton construction (based on the PC-stable algorithm).</p> <p>Stage-I begins with a complete undirected graph and performs conditional independence tests to delete the edges between independent variables pairs, reducing the computational cost of subsequent regressions and independence tests.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>ndarray</code> <p>The observational dataset shown as a matrix or table, with a format of \"sample (n) * dimension (d).\" (input as Pandas dataframe is also acceptable)</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Update <code>_skeleton</code> as the estimated undirected graph corresponding to the causal graph, initialize <code>_adjacency_matrix</code> via a copy of <code>_skeleton</code>, and record <code>_stage1_time</code> as the stage-1 computational time.</p> Source code in <code>cadimulc\\hybrid_algorithms\\hybrid_algorithms.py</code> <pre><code>def _stage_1_learning(self, dataset: ndarray) -&gt; object:\n    \"\"\"\n    **Stage-I**: Causal skeleton construction (based on the PC-stable algorithm).\n\n    Stage-I begins with a complete undirected graph and performs **conditional\n    independence tests** to delete the edges between independent variables pairs,\n    reducing the computational cost of subsequent regressions and independence tests.\n\n    Parameters:\n        dataset:\n            The observational dataset shown as a matrix or table,\n            with a format of \"sample (n) * dimension (d).\"\n            (input as Pandas dataframe is also acceptable)\n\n    Returns:\n        self:\n            Update `_skeleton` as the estimated undirected graph corresponding to\n            the causal graph, initialize `_adjacency_matrix` via a copy of `_skeleton`,\n            and record `_stage1_time` as the stage-1 computational time.\n    \"\"\"\n\n    self._causal_skeleton_learning(dataset)\n\n    return self\n</code></pre> Private Method: _stage_2_learning <p>Stage-II: Partial causal order identification.</p> <p>Based on the causal skeleton by stage-I, stage II in MLC-LiNGAM identifies causal directions among the adjacent variables that are implied by the skeleton. Causal orders relative to all variables can be partially determined by regression and independence tests, if variables that are relatively exogenous or endogenous do not be affected by latent confounders.</p> <p>Parameters:</p> Name Type Description Default <code>graph_pattern_manager</code> <p>An auxiliary module embedded in the MLC-LiNGAM algorithm, managing adjacency matrices amidst the procedure between causal skeleton learning and causal direction orientation.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Update <code>_adjacency_matrix</code> as the estimated (partial) directed acyclic graph (DAG) corresponding to the causal graph, and <code>record _stage2_time</code> as the <code>stage-2</code> computational time.</p> Source code in <code>cadimulc\\hybrid_algorithms\\hybrid_algorithms.py</code> <pre><code>def _stage_2_learning(self, graph_pattern_manager) -&gt; object:\n    \"\"\"\n    **Stage-II**: Partial causal order identification.\n\n    Based on the causal skeleton by stage-I,\n    stage II in *MLC-LiNGAM* identifies causal directions among the adjacent variables\n    that are implied by the skeleton.\n    Causal orders relative to all variables can be partially determined by **regression\n    and independence tests**, if variables that are relatively exogenous or endogenous\n    do not be affected by latent confounders.\n\n    Parameters:\n        graph_pattern_manager:\n            An auxiliary module embedded in the MLC-LiNGAM algorithm,\n            managing adjacency matrices amidst the procedure between causal skeleton\n            learning and causal direction orientation.\n\n    Returns:\n        self:\n            Update `_adjacency_matrix` as the estimated (partial) directed acyclic\n            graph (DAG) corresponding to the causal graph,\n            and `record _stage2_time` as the `stage-2` computational time.\n    \"\"\"\n\n    # Arguments for testing:\n    #   _skeleton(attribute): ndarray\n    #   _dataset(attribute): ndarray\n    #   _dim(attribute): int\n\n    start = time.perf_counter()\n\n    # Reconstruction of the causal skeleton entails specific pairs of adjacent variables,\n    # rather than all pairs of variables.\n    causal_skeleton = self._skeleton\n\n    # MLC-LiNGAM performs regression and independence tests efficiently\n    # based on the adjacency set.\n    adjacent_set = GraphPatternManager.find_adjacent_set(\n        causal_skeleton=causal_skeleton\n    )\n\n    # Apply Algorithm-2 (given by the MLC-LiNGAM algorithm).\n    self._algorithm_2(\n        corresponding_adjacent_set=adjacent_set,\n        corresponding_dataset=cp.copy(self._dataset),\n        corresponding_variables=np.arange(self._dim),\n        graph_pattern_manager=graph_pattern_manager\n    )\n\n    # Record computational time.\n    end = time.perf_counter()\n    self._stage2_time = end - start\n\n    return self\n</code></pre> Private Method: _stage_3_learning <p>Stage-III: Latent confounders' detection</p> <p>Stage-III will learn more causal orders if some variables are not affected by the latent confounders but are in the remaining subset. Meanwhile, the stage-III learning makes use of the causal skeleton information to reduce the testing space of remaining variables from all subsets to typical maximal cliques.</p> <p>Notice that the maximal cliques, including the undirected relations that cannot be determined, are possibly formed by latent confounders. This in turn provides insight to detect the latent confounders, and uncover the causal relations among observed and latent variables.</p> <p>Parameters:</p> Name Type Description Default <code>graph_pattern_manager</code> <p>An auxiliary module embedded in the MLC-LiNGAM algorithm, featuring the algorithmic behavior of the maximal-cliques pattern recognition.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Update <code>_adjacency_matrix</code> as the estimated (partial) directed acyclic graph (DAG) corresponding to the causal graph, <code>_latent_confounder_detection</code> as the undirected maximal cliques after  stage-III learning, and <code>record _stage3_time</code> as the <code>stage-3</code>  computational time.</p> Source code in <code>cadimulc\\hybrid_algorithms\\hybrid_algorithms.py</code> <pre><code>def _stage_3_learning(self, graph_pattern_manager) -&gt; object:\n    \"\"\"\n    **Stage-III**: Latent confounders' detection\n\n    Stage-III will learn more causal orders if some variables are not affected\n    by the latent confounders but are in the remaining subset.\n    Meanwhile, the stage-III learning makes use of the causal skeleton information\n    to reduce the testing space of remaining variables from all subsets to typical\n    **maximal cliques**.\n\n    Notice that the maximal cliques, including the undirected relations that cannot\n    be determined, are possibly formed by latent confounders. This in turn provides\n    insight to detect the latent confounders, and uncover the causal relations\n    among observed and latent variables.\n\n    Parameters:\n        graph_pattern_manager:\n            An auxiliary module embedded in the MLC-LiNGAM algorithm,\n            featuring the algorithmic behavior of the maximal-cliques pattern recognition.\n\n    Returns:\n        self:\n            Update ``_adjacency_matrix`` as the estimated (partial) directed acyclic\n            graph (DAG) corresponding to the causal graph,\n            ``_latent_confounder_detection`` as the undirected maximal cliques after\n             stage-III learning, and `record _stage3_time` as the `stage-3`\n             computational time.\n    \"\"\"\n\n    # Arguments for testing:\n    #   _skeleton (attribute)\n    #   _adjacency_matrix (attribute)\n    #   _parents_set (attribute)\n    #   _dataset(attribute): ndarray\n\n    start = time.perf_counter()\n\n    # Recognize the maximal-clique pattern based on the causal skeleton.\n    maximal_cliques_completely_undetermined = (\n        GraphPatternManager.recognize_maximal_cliques_pattern(\n            causal_skeleton=self._skeleton,\n            adjacency_matrix=self._adjacency_matrix\n        )\n    )\n\n    # Setup of regression, referring to MLC-LiNGAM default settings.\n    regressor = LinearRegression()\n    residuals_dataset = cp.copy(self._dataset)\n\n    # Replace the variables in the clique with their corresponding residuals via\n    # regressing out the effect of their confounded parents that are outside the clique.\n    for maximal_clique in maximal_cliques_completely_undetermined:\n        # Record: Each of the variable requires a single replacement if necessary.\n        variables_replaced = {}\n        for variable in maximal_clique:\n            variables_replaced[variable] = set()\n\n        # Get undetermined pairs within a clique.\n        for i in maximal_clique:\n            for j in maximal_clique[maximal_clique.index(i) + 1:]:\n                parents_i = graph_pattern_manager.managing_parents_set[i]\n                parents_j = graph_pattern_manager.managing_parents_set[j]\n\n                # Conduct residuals replacement if the variables share the same parents.\n                if (parents_i &amp; parents_j) != set():\n                    confounded_parents = parents_i &amp; parents_j\n\n                    for confounder in confounded_parents:\n                        data_confounder = residuals_dataset[:, confounder]\n\n                        if confounder not in variables_replaced[i]:\n                            variables_replaced[i].add(confounder)\n\n                            data_i = residuals_dataset[:, i]\n                            residuals_i = get_residuals_scm(\n                                explanatory_data=data_confounder,\n                                explained_data=data_i,\n                                regressor=regressor\n                            )\n                            residuals_dataset[:, i] = residuals_i.squeeze()\n\n                        if confounder not in variables_replaced[j]:\n                            variables_replaced[j].add(confounder)\n\n                            data_j = residuals_dataset[:, j]\n                            residuals_j = get_residuals_scm(\n                                explanatory_data=data_confounder,\n                                explained_data=data_j,\n                                regressor=regressor\n                            )\n                            residuals_dataset[:, j] = residuals_j.squeeze()\n\n    # Apply Algorithm-2 on the maximal cliques.\n    for maximal_clique in maximal_cliques_completely_undetermined:\n        # Get adjacent set with respect to the variables within maximal cliques.\n        adjacent_set_clique = {}\n        for variable in maximal_clique:\n            adjacent_set_clique[variable] = set(maximal_clique) - {variable}\n\n        # Apply Algorithm-2 (given by the MLC-LiNGAM algorithm).\n        self._algorithm_2(\n            corresponding_adjacent_set=adjacent_set_clique,\n            corresponding_dataset=residuals_dataset,\n            corresponding_variables=np.array(maximal_clique),\n            graph_pattern_manager=graph_pattern_manager,\n            _specify_adjacency=True,\n            _adjacent_set=adjacent_set_clique\n        )\n\n    # Update latent confounder detection\n    graph_pattern_manager.store_last_managing_adjacency_matrix()\n    self._latent_confounder_detection = (\n        graph_pattern_manager.get_undetermined_cliques(\n            maximal_cliques=maximal_cliques_completely_undetermined\n        )\n    )\n\n    # Record computational time.\n    end = time.perf_counter()\n    self._stage3_time = end - start\n\n    return self\n</code></pre> Class: Nonlinear-MLC <sup>[2]</sup> Primary Method: fit <p>Fitting data via the Nonlinear-MLC causal discovery algorithm.</p> <p>The procedure comprises the causal skeleton learning in the initial stage, along with the causal identification procedure involving non-linear regression and independence tests for the subsequence. Following the well-known divide-and-conquer strategy, non-linear causal inference are conducted over the maximal cliques recognized from the estimated causal skeleton.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>ndarray</code> <p>The observational dataset shown as a matrix or table, with a format of \"sample (n) * dimension (d).\" (input as Pandas dataframe is also acceptable)</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Update the <code>adjacency_matrix</code> represented as an estimated causal graph. The <code>adjacency_matrix</code> is a (d * d) numpy array with 0/1 elements characterizing the causal direction.</p> Source code in <code>cadimulc\\hybrid_algorithms\\hybrid_algorithms.py</code> <pre><code>def fit(self, dataset: ndarray) -&gt; object:\n    \"\"\"\n    Fitting data via the *Nonlinear-MLC* causal discovery algorithm.\n\n    The procedure comprises the **causal skeleton learning** in the initial stage,\n    along with the causal identification procedure involving **non-linear regression**\n    and **independence tests** for the subsequence.\n    Following the well-known **divide-and-conquer** strategy, non-linear causal inference\n    are conducted over the maximal cliques recognized from the estimated causal skeleton.\n\n    Parameters:\n        dataset:\n            The observational dataset shown as a matrix or table,\n            with a format of \"sample (n) * dimension (d).\"\n            (input as Pandas dataframe is also acceptable)\n\n    Returns:\n        self:\n            Update the ``adjacency_matrix`` represented as an estimated causal graph.\n            The ``adjacency_matrix`` is a (d * d) numpy array with 0/1 elements\n            characterizing the causal direction.\n    \"\"\"\n\n    start = time.perf_counter()\n\n    # Reconstruct a causal skeleton using the PC-stable algorithm.\n    self._causal_skeleton_learning(dataset)\n\n    # Recognize the maximal-clique pattern based on the causal skeleton.\n    maximal_cliques = GraphPatternManager.recognize_maximal_cliques_pattern(\n        causal_skeleton=self._skeleton\n    )\n\n    # Initialize a graph pattern manager for subsequent learning.\n    graph_pattern_manager = GraphPatternManager(\n        init_graph=self._skeleton\n    )\n\n    # Perform the nonlinear-mlc causal discovery.\n    continue_search = True\n    while continue_search:\n\n        # Obtain the cliques that remain at least one edge undetermined.\n        undetermined_maximal_cliques = (\n            graph_pattern_manager.get_undetermined_cliques(maximal_cliques)\n        )\n\n        # End if all edges over the cliques have been determined.\n        if len(undetermined_maximal_cliques) == 0:\n            break\n\n        # Temporally store the adjacency matrix ahead of a search round.\n        graph_pattern_manager.store_last_managing_adjacency_matrix()\n\n        # In light of the L-ANMs theory (proposed in paper), start the search round\n        # by conducting non-linear causal inference based on maximal cliques.\n        determined_pairs = self._clique_based_causal_inference(\n            undetermined_maximal_cliques=undetermined_maximal_cliques\n        )\n\n        # Orient determined causal directions after a search round over maximal cliques.\n        graph_pattern_manager.identify_directed_causal_pair(\n            determined_pairs=determined_pairs\n        )\n\n        # Update the causal adjacency matrix and parent-relations set\n        # after a search round over maximal cliques.\n        self._adjacency_matrix = (\n            graph_pattern_manager.managing_adjacency_matrix\n        )\n        self._parents_set = (\n            graph_pattern_manager.managing_parents_set\n        )\n\n        # Check if new causal relations have been determined\n        # after the last round searching.\n        newly_determined = (\n            graph_pattern_manager.check_newly_determined(\n                undetermined_maximal_cliques\n            )\n        )\n\n        # End if there is none of new causal relation advancing the further search.\n        if not newly_determined:\n            continue_search = False\n\n    end = time.perf_counter()\n\n    self._running_time += (end - start)\n\n    return self\n</code></pre> Private Method: _clique_based_causal_inference <p>For each of the undetermined maximal cliques (e.g. at least one edge within a maximal clique remains undirected) with respect to the whole maximal-clique patterns, the algorithm conducts non-linear regression and independence tests with the additional explanatory variables selected from the undetermined maximal clique.</p> <p>This strategy is argued to enhance the efficiency and robustness as to the non-linear causal discovery with multiple latent confounders, serving as the essence of the Nonlinear-MLC algorithm (See \"Latent-ANMs Lemma\" for details in the relevant paper, Section 3).</p> <p>Parameters:</p> Name Type Description Default <code>undetermined_maximal_cliques</code> <code>list[list]</code> <p>A list of undetermined maximal cliques in which the element (maximal clique) involves at least one edge remaining undirected (e.g. <code>[[X, Y, Z]]</code> could stand for a maximal clique , with both determined and undetermined relations \"  'X &lt;-&gt; Y', 'Y &lt;-&gt; Z', and 'X -&gt; Z'\"). required <p>Returns:</p> Type Description <code>list</code> <p>The list of determined pairs over the inputting undetermined cliques after searching (e.g. <code>[[X, Y], [Y, Z]]</code> stands for two of the determined pairs \"X -&gt; Y\" and \"Y -&gt; Z\" after searching).</p> Source code in <code>cadimulc\\hybrid_algorithms\\hybrid_algorithms.py</code> <pre><code>def _clique_based_causal_inference(\n        self,\n        undetermined_maximal_cliques: list[list]\n) -&gt; list:\n    \"\"\"\n    For each of the undetermined maximal cliques (e.g. at least one edge\n    within a maximal clique remains undirected) with respect to the whole\n    maximal-clique patterns,\n    the algorithm conducts non-linear regression and independence tests with\n    the **additional explanatory variables** selected from the\n    undetermined maximal clique.\n\n    This strategy is argued to enhance the **efficiency** and **robustness** as to\n    the **non-linear causal discovery with multiple latent confounders**,\n    serving as the essence of the *Nonlinear-MLC* algorithm (See \"Latent-ANMs Lemma\" for\n    details in the [relevant paper](https://xuanzhichen.github.io/work/papers/nonlinear_mlc.pdf),\n    Section 3).\n\n    Parameters:\n        undetermined_maximal_cliques:\n            A list of undetermined maximal cliques in which the element\n            (maximal clique) involves at least one edge remaining undirected\n            (e.g. `[[X, Y, Z]]` could stand for a maximal clique &lt;X, Y, Z&gt;,\n            with both determined and undetermined relations \"\n             'X &lt;-&gt; Y', 'Y &lt;-&gt; Z', and 'X -&gt; Z'\").\n\n    Returns:\n        The list of determined pairs over the inputting undetermined cliques\n         after searching\n         (e.g. `[[X, Y], [Y, Z]]` stands for two of the determined pairs\n         \"X -&gt; Y\" and \"Y -&gt; Z\" after searching).\n    \"\"\"\n\n    # Arguments for Testing:\n    #     _adjacency_matrix(attribute): ndarray\n    #     _dataset(attribute): ndarray\n    #     _parents_set(attribute): dict\n\n    determined_pairs = []\n\n    # Conduct non-linear causal inference based on each maximal clique unit.\n    for undetermined_maximal_clique in undetermined_maximal_cliques:\n\n        # Initialize the lists with elements of undetermined causal relations.\n        # e.g. the element (cause, effect) specifies \"cause -&gt; effect\"\n        undetermined_pairs = []\n\n        # Get undetermined pairs within a clique.\n        for i in undetermined_maximal_clique:\n            for j in undetermined_maximal_clique[\n                undetermined_maximal_clique.index(i) + 1:\n            ]:\n                if (self._adjacency_matrix[i][j] == 1) and (\n                    self._adjacency_matrix[j][i] == 1\n                ):\n                    undetermined_pairs.append([i, j])\n\n        # Conduct pairwise non-linear regression and independence tests.\n        for pair in undetermined_pairs:\n            determined = False\n\n            p_value_max = self.pc_alpha\n            causation = copy_and_rename(pair)\n\n            # Unravel the pairwise inferred directions respectively.\n            pair_temp = cp.copy(pair)\n            pair_temp.reverse()\n            pair_reversed = copy_and_rename(pair_temp)\n\n            for cause, effect in zip(pair, pair_reversed):\n\n                # ================= Empirical Regressor Construction ==================\n\n                # initialization of explanatory-and-explained variables\n                explanatory_vars = set()\n                explained_var = set()\n\n                # basic explanatory-and-explained variables: cause-effect\n                explanatory_vars.add(cause)\n                explained_var.add(effect)  # namely the effect variable\n\n                # Add explanatory variables to strengthen empirical regression:\n\n                # determined parent-relations amidst the algorithm memory\n                explanatory_vars = explanatory_vars | set(self._parents_set[effect])\n\n                # undetermined connections within the maximal clique\n                explanatory_vars = explanatory_vars | (\n                        set(undetermined_maximal_clique) - {effect}\n                )\n\n                # Regress the effect variable on empirical explanatory variables\n                # (in an attempt to cancel unobserved confounding).\n\n                explanatory_data = cp.copy(\n                    self._dataset[:, list(explanatory_vars)]\n                )\n\n                # namely the data with respect to the effect variable\n                explained_data = cp.copy(\n                    self._dataset[:, list(explained_var)]\n                )\n\n                # regressing residuals via fitting SCMs\n\n                # Development notes:\n                # The following IF branch is added due to a bug, occurring when\n                # reinitializing the regressor (GAM) instance for fitting pairwise data.\n                if explanatory_data.shape[1] == 1:\n                    explanatory_data = check_1dim_array(explanatory_data)\n                    explained_data = check_1dim_array(explained_data)\n\n                    regressor = LinearGAM()\n                    regressor.fit(explanatory_data, explained_data)\n                    est_explained_data = regressor.predict(explanatory_data)\n                    est_explained_data = check_1dim_array(est_explained_data)\n\n                    residuals = explained_data - est_explained_data\n\n                else:\n                    residuals = get_residuals_scm(\n                        explanatory_data=explanatory_data,\n                        explained_data=explained_data,\n                        regressor=self.regressor\n                    )\n\n                # Remove effects of parent-relations from the cause variable\n                # (in an attempt to cancel unobserved confounding).\n\n                cause_parents = list(self._parents_set[cause])\n\n                if len(cause_parents) &gt; 0:\n                    # Development notes:\n                    # The following IF branch is added due to a bug, occurring when\n                    # reinitializing the regressor (GAM) instance for fitting pairwise data.\n                    if len(cause_parents) == 1:\n                        cause_explanatory_data = check_1dim_array(\n                            cp.copy(self._dataset[:, cause_parents])\n                        )\n                        cause_explained_data = check_1dim_array(\n                            cp.copy(self._dataset[:, cause])\n                        )\n\n                        regressor = LinearGAM()\n                        regressor.fit(cause_explanatory_data, cause_explained_data)\n                        est_cause_explained_data = regressor.predict(cause_explanatory_data)\n                        est_cause_explained_data = check_1dim_array(est_cause_explained_data)\n\n                        cause_residuals = cause_explained_data - est_cause_explained_data\n                        cause_data = copy_and_rename(cause_residuals)\n                    else:\n                        cause_data = get_residuals_scm(\n                            explanatory_data=self._dataset[:, cause_parents],\n                            explained_data=self._dataset[:, cause],\n                            regressor=self.regressor\n                        )\n                else:\n                    cause_data = cp.copy(self._dataset[:, cause])\n\n                # ========================== Independence Test ========================\n\n                # Conduct the independence test\n                # between the cause variable and regressing residuals.\n                p_value = conduct_ind_test(\n                    explanatory_data=cause_data,\n                    residuals=residuals,\n                    ind_test_method=self.ind_test\n                )\n\n                # One single inferred causal direction is determined given the\n                # maximal p-value exceeding the threshold of the significant level.\n                if p_value &gt; p_value_max:\n                    determined = True\n\n                    p_value_max = p_value\n                    causation = (cause, effect)\n\n            if determined:\n                determined_pairs.append(causation)\n\n    return determined_pairs\n</code></pre> Auxiliary Class: GraphPatternManager Primary Method: recognize_maximal_cliques_pattern <p>Recognize the maximal-cliques pattern based on a causal skeleton (undirected graph) or a partial causal adjacency matrix (partially directed graph).</p> <p>Reference by <code>recognize_maximal_cliques_pattern</code></p> <p>Bron, Coen, and Joep Kerbosch. \"Algorithm 457: finding all cliques of an undirected graph.\" Communications of the ACM 16, no. 9 (1973): 575-577. (Implementation by NetworkX)</p> <p>Parameters:</p> Name Type Description Default <code>causal_skeleton</code> <code>ndarray</code> <p>The undirected graph corresponding to the causal graph.</p> required <code>adjacency_matrix</code> <code>ndarray | None</code> <p>The (partially) directed acyclic graph (DAG) corresponding to the causal graph.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>maximal_cliques</code> <code>list[list]</code> <p>A whole list of \"maximal cliques\", along with each of the \"maximal clique\" element that is as well in form of list.</p> Source code in <code>cadimulc\\hybrid_algorithms\\hybrid_algorithms.py</code> <pre><code>@staticmethod\ndef recognize_maximal_cliques_pattern(\n        causal_skeleton: ndarray,\n        adjacency_matrix: ndarray | None = None\n) -&gt; list[list]:\n    \"\"\"\n    Recognize the maximal-cliques pattern based on a causal skeleton (undirected graph)\n    or a partial causal adjacency matrix (partially directed graph).\n\n    !!! note \"Reference by ``recognize_maximal_cliques_pattern``\"\n        Bron, Coen, and Joep Kerbosch.\n        \"Algorithm 457: finding all cliques of an undirected graph.\"\n        Communications of the ACM 16, no. 9 (1973): 575-577.\n        (Implementation by [NetworkX](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.clique.find_cliques.html))\n\n    Parameters:\n        causal_skeleton:\n            The undirected graph corresponding to the causal graph.\n        adjacency_matrix:\n            The (partially) directed acyclic graph (DAG) corresponding to the causal graph.\n\n    Returns:\n        maximal_cliques:\n            A whole list of \"maximal cliques\", along with each of the\n            \"maximal clique\" element that is as well in form of list.\n    \"\"\"\n    # Remove the edge that has been determined as to the skeleton\n    if adjacency_matrix is not None:\n        dim = adjacency_matrix.shape[0]\n        for i in range(dim):\n            for j in range(dim):\n                if (adjacency_matrix[i][j] == 1) and (adjacency_matrix[j][i] == 0):\n                    causal_skeleton[i][j] = 0\n                    causal_skeleton[j][i] = 0\n\n    # Search maximal cliques by the Bron-Kerbosch algorithm.\n    undirected_graph_nx = nx.from_numpy_array(causal_skeleton)\n    clique_iter = nx.find_cliques(undirected_graph_nx)\n    maximal_cliques_temp = [clique for clique in clique_iter]\n\n    # Remove the trivial graph from maximal_cliques.\n    maximal_cliques = []\n    for clique in maximal_cliques_temp:\n        if len(clique) &gt; 1:\n            maximal_cliques.append(clique)\n\n    return maximal_cliques\n</code></pre> Primary Method: find_adjacent_set <p>Given a causal skeleton (or a subset of the causal skeleton), find out the adjacent variables for each of the variable.</p> <p>Parameters:</p> Name Type Description Default <code>causal_skeleton</code> <code>ndarray</code> <p>The undirected graph corresponding to the causal graph.</p> required <p>Returns:</p> Name Type Description <code>adjacent_set</code> <code>dict</code> <p>A dictionary that describes the adjacency relations: <code>{variable: (adjacent variables)}</code>.</p> Source code in <code>cadimulc\\hybrid_algorithms\\hybrid_algorithms.py</code> <pre><code>@staticmethod\ndef find_adjacent_set(causal_skeleton: ndarray) -&gt; dict:\n    \"\"\"\n    Given a causal skeleton (or a subset of the causal skeleton),\n    find out the adjacent variables for each of the variable.\n\n    Parameters:\n        causal_skeleton:\n            The undirected graph corresponding to the causal graph.\n\n    Returns:\n        adjacent_set:\n            A dictionary that describes the adjacency relations:\n            `{variable: (adjacent variables)}`.\n    \"\"\"\n\n    dim = causal_skeleton.shape[1]\n    adjacent_set = {}\n\n    for i in range(dim):\n        adjacent_set[i] = set()\n\n    for i in range(dim):\n        for j in range(dim):\n            if i != j:\n                if causal_skeleton[i][j] == 1:\n                    adjacent_set[i].add(j)\n\n    return adjacent_set\n</code></pre> Base Class: HybridFrameworkBase Primary Method: _causal_skeleton_learning <p>Causal skeleton construction (based on the PC-stable algorithm).</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>ndarray</code> <p>The observational dataset shown as a matrix or table, with a format of \"sample (n) * dimension (d).\" (input as Pandas dataframe is also acceptable)</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Update <code>_skeleton</code> as the estimated undirected graph corresponding to the causal graph, initialize <code>_adjacency_matrix</code> via a copy of <code>_skeleton</code>, and record <code>_stage1_time</code> as the stage-1 computational time (causal skeleton learning is usually the first stage in hybrid-based causal discovery algorithm) .</p> Source code in <code>cadimulc\\hybrid_algorithms\\hybrid_framework.py</code> <pre><code>def _causal_skeleton_learning(self, dataset: ndarray) -&gt; object:\n    \"\"\"\n    Causal skeleton construction (based on the PC-stable algorithm).\n\n    Parameters:\n        dataset:\n            The observational dataset shown as a matrix or table,\n            with a format of \"sample (n) * dimension (d).\"\n            (input as Pandas dataframe is also acceptable)\n\n    Returns:\n        self:\n            Update `_skeleton` as the estimated undirected graph corresponding to\n            the causal graph, initialize `_adjacency_matrix` via a copy of `_skeleton`,\n            and record `_stage1_time` as the stage-1 computational time\n            (causal skeleton learning is usually the first stage in\n            hybrid-based causal discovery algorithm) .\n    \"\"\"\n\n    # Arguments for testing:\n    #   pc_alpha(parameter): float\n    #   _dataset(attribute): dataframe\n\n    self._dim = dataset.shape[1]\n    self._dataset = dataset\n    for i in range(self._dim):\n        self._parents_set[i] = set()\n\n    data = cp.copy(self._dataset)\n\n    # Development notes:\n    # Unify the linear independence test even for nonlinear-mlc.\n    skeleton, running_time = get_skeleton_from_pc(\n        data=data,\n        alpha=self.pc_alpha,\n        ind_test_type='linear'\n    )\n\n    self._skeleton = cp.copy(skeleton)\n    self._adjacency_matrix = cp.copy(skeleton)\n    self._stage1_time = running_time\n\n    return self\n</code></pre>"},{"location":"hybrid_algorithms/#cadimulc.hybrid_algorithms.hybrid_algorithms.MLCLiNGAM","title":"<code>cadimulc.hybrid_algorithms.hybrid_algorithms.MLCLiNGAM</code>","text":"<p>             Bases: <code>HybridFrameworkBase</code></p> <p>MLC-LiNGAM stands for a hybrid causal discovery method for the LiNGAM approach with multiple latent confounders. It serves as an enhancement of LiNGAM<sup>*</sup> via combining the advantages of constraint-based and functional-based causality methodology.</p> <p>The LiNGAM causal discovery approach</p> <p>LiNGAM, the linear non-Gaussian acyclic model, is known as one of the structural-identifiable SCMs.</p> <p>MLC-LiNGAM was proposed to alleviate the following issues:</p> <ul> <li>how to detect the latent confounders;</li> <li>how to uncover the causal relations among observed and latent variables.</li> </ul>"},{"location":"hybrid_algorithms/#cadimulc.hybrid_algorithms.hybrid_algorithms.MLCLiNGAM.__init__","title":"<code>__init__(pc_alpha=0.05, _latent_confounder_detection=[])</code>","text":"<p>Parameters:</p> Name Type Description Default <code>pc_alpha</code> <code>float</code> <p>Significance level of independence tests (p_value), which is required by the constraint-based methodology incorporated in the initial stage of the hybrid causal discovery framework.</p> <code>0.05</code>"},{"location":"hybrid_algorithms/#cadimulc.hybrid_algorithms.hybrid_algorithms.NonlinearMLC","title":"<code>cadimulc.hybrid_algorithms.hybrid_algorithms.NonlinearMLC</code>","text":"<p>             Bases: <code>HybridFrameworkBase</code></p> <p>The hybrid algorithm Nonlinear-MLC, incorporation of the constraint-based and functional-based causal discovery methodology, is developed for the general causal inference over non-linear data in presence of multiple unknown factors.</p> <p>A primary feature of Nonlinear-MLC lies in exploiting the non-linear causal identification with multiple latent confounders (proposed as the Latent-ANMs causal identification), which is on the basic of the well-known ANMs<sup>*</sup> method.</p> <p>The ANMs causal discovery approach</p> <p>ANMs, the additive-noise-models, is known as one of the structural-identifiable SCMs.</p>"},{"location":"hybrid_algorithms/#cadimulc.hybrid_algorithms.hybrid_algorithms.NonlinearMLC.__init__","title":"<code>__init__(ind_test='kernel_ci', pc_alpha=0.05, _regressor=LinearGAM())</code>","text":"<p>Parameters:</p> Name Type Description Default <code>ind_test</code> <code>str</code> <p>Popular non-linear independence-tests methods are recommended: Kernel-based Conditional Independence tests (KCI); Hilbert-Schmidt Independence Criterion for General Additive Models (HSIC-GAMs).</p> <code>'kernel_ci'</code> <code>pc_alpha</code> <code>float</code> <p>Significance level of independence tests (p_value), which is required by the constraint-based methodology incorporated in the initial stage of the hybrid causal discovery framework.</p> <code>0.05</code>"},{"location":"hybrid_algorithms/#cadimulc.hybrid_algorithms.hybrid_algorithms.GraphPatternManager","title":"<code>cadimulc.hybrid_algorithms.hybrid_algorithms.GraphPatternManager</code>","text":"<p>             Bases: <code>object</code></p> <p>An auxiliary module embedded in the MLC-LiNGAM and Nonlinear-MLC algorithms, featuring the algorithmic behavior of the maximal-cliques pattern recognition.</p> <p>The module as well manages adjacency matrices amidst the procedure between causal skeleton learning and causal direction orientation.</p>"},{"location":"hybrid_algorithms/#cadimulc.hybrid_algorithms.hybrid_framework.HybridFrameworkBase","title":"<code>cadimulc.hybrid_algorithms.hybrid_framework.HybridFrameworkBase</code>","text":"<p>A hybrid causal discovery framework with established implementations of discovering the causal skeleton by the Peter-Clark algorithm (PC algorithm). The framework is incorporated into the initial stage of both the Nonlinear-MLC and MLC-LiNGAM causal discovery algorithms.</p>"},{"location":"hybrid_algorithms/#cadimulc.hybrid_algorithms.hybrid_framework.HybridFrameworkBase.__init__","title":"<code>__init__(pc_alpha=0.5, _dataset=None, _dim=None, _skeleton=None, _adjacency_matrix=None, _parents_set={})</code>","text":"<p>Parameters:</p> Name Type Description Default <code>pc_alpha</code> <code>float</code> <p>Significance level of independence tests (p_value), which is required by the constraint-based methodology incorporated in the initial stage of the hybrid causal discovery framework.</p> <code>0.5</code> <code>_dataset</code> <code>ndarray</code> <p>The observational dataset shown as a matrix or table, with a format of \"sample (n) * dimension (d).\" (input as Pandas dataframe is also acceptable)</p> <code>None</code> <code>_dim</code> <code>int</code> <p>int The variable dimension corresponding to the causal graph.</p> <code>None</code> <code>_skeleton</code> <code>ndarray</code> <p>The estimated undirected graph corresponding to the causal graph.</p> <code>None</code> <code>_adjacency_matrix</code> <code>ndarray</code> <p>The estimated directed acyclic graph (DAG) corresponding to the causal graph.</p> <code>None</code> <code>_parents_set</code> <code>dict</code> <p>dict The child-parents relations associating with the adjacency matrix.</p> <code>{}</code>"},{"location":"hybrid_algorithms/#reference","title":"Reference","text":"<p>[1] Chen, Wei, Ruichu Cai, Kun Zhang, and Zhifeng Hao. \"Causal discovery in linear non-gaussian acyclic model with multiple latent confounders. \" IEEE Transactions on Neural Networks and Learning Systems. 2021.</p> <p>[2] Chen, Xuanzhi, Wei Chen, Ruichu Cai.  \"Non-linear Causal Discovery for Additive Noise Model with     Multiple Latent Confounders\". Xuanzhi's Personal Website. 2023.</p> <p>[3] Shimizu, Shohei, Patrik O. Hoyer, Aapo Hyv\u00e4rinen, Antti Kerminen, and Michael Jordan.  \"A linear non-Gaussian acyclic model for causal discovery.\"  Journal of Machine Learning Research. 2006.</p> <p>[4] Hoyer, Patrik, Dominik Janzing, Joris M. Mooij, Jonas Peters, and Bernhard Sch\u00f6lkopf. \"Nonlinear causal discovery with additive noise models.\"  Advances in neural information processing systems. 2008.</p> <p>[5] Spirtes, Peter, Clark N. Glymour, and Richard Scheines. Causation, prediction, and search. MIT press, 2000.</p>"},{"location":"quick_tutorials/","title":"Quick Tutorials","text":"<p>Here is a simple example to help you take a look on the \"micro\" workflow of the  causal discovery task. Attaching to this \"mini\" procedure by CADIMULC,  you would experience how to complete the basic causal discovery in minutes.</p>"},{"location":"quick_tutorials/#1-installation","title":"1. Installation","text":"<p>To install CADIMULC, run the following command from your Python virtual environment:</p> <pre><code>pip install cadimulc\n</code></pre>"},{"location":"quick_tutorials/#2-hybrid-based-causal-discovery","title":"2. Hybrid-Based Causal Discovery","text":"<p>Causal discovery algorithms (<code>MLCLiNGAM</code>, <code>NonlinearMLC</code>) in CADIMULC provide you an out-of-box hybrid framework to combine  the two lines of state-of-art causal structure learning methodology:  constraint-based methods and functional-based methods. <pre><code># hybrid-algorithms based on data assumptions: linearity or non-linearity\nfrom cadimulc.hybrid_algorithms import MLCLiNGAM, NonlinearMLC\n</code></pre></p>"},{"location":"quick_tutorials/#22-data-fitting","title":"2.2 Data Fitting","text":"<p>Applying the hybrid-based approaches is super easy. Take <code>NonlinearMLC</code>. <pre><code>nonlinear_mlc = NonlinearMLC()\n</code></pre></p> <p>We have adjusted and set up for the technical parameters in hybrid-based causal discovery, with respect to  the conditional-independence-test (CIT) type in constraint-based methods,  and the functional-causal-models (FCMs) type in functional-based methods. <pre><code># NonlinearMLC uses the well-known kernel-based CIT for nonlinear data. \nnonlinear_mlc.ind_test\n&gt;&gt;&gt; 'kernel_ci'  \n\n# NonlinearMLC uses the well-known general additive models for FCMs regression.\nnonlinear.regressor\n&gt;&gt;&gt; LinearGAM()  \n</code></pre></p> <p>We can directly conduct non-linear causal discovery on the <code>dataset</code> we want to analyze. <pre><code># Perform the Nonlinear-MLC causal discovery algorithm.\nnonlinear_mlc.fit(dataset=dataset)\n</code></pre> That's it. The <code>fit</code> procedure has involved a two-steps hybrid causal discovery strategy. </p> <p>However, if you would like to presume linearity relations entailed by the <code>dataset</code>, you could simply replace <code>NonlinearMLC</code> as <code>MLCLiNGAM</code>, and repeat the same steps.</p> <p><pre><code># Perform the MLC-LiNGAM causal discovery algorithm.\nmlc_lingam = MLCLiNGAM()\nmlc_lingam.fit(dataset=dataset)\n</code></pre> For technical usages and theoretical details, please refer to Hybrid-Based Approaches.</p>"},{"location":"quick_tutorials/#22-simple-visualization","title":"2.2 Simple Visualization","text":"<p>CADIMULC offers a simple function to plot the causal graph discovered by the algorithms. <pre><code>from cadimulc.utils.visualization import draw_graph_from_ndarray\nimport matplotlib.pyplot as plt\n</code></pre> The causal graph is stored in form of an adjacency matrix inside the algorithm instance, so you need to input that adjacency matrix to the plot function. <pre><code>array = nonlinear_mlc.adjacency_matrix_\ndraw_graph_from_ndarray(array)\nplt.show()\n</code></pre></p>"},{"location":"quick_tutorials/#3-data-generation","title":"3. Data Generation","text":"<p>Data Generation in step-3 is not a must,  we can directly apply the algorithm in step-2 for established datasets. Tutorials for step-3 emphasizes that the data generation setting essentially reflects our priori causal assumption. </p> <p>Nevertheless, the assumption does not necessarily hold over the established dataset we want to analyze.  Keeping this in mind might be helpful for us to objectively interpret the hypothetical causation learned from the empirical data.</p>"},{"location":"quick_tutorials/#31-simple-clique-structure","title":"3.1 Simple Clique Structure","text":"<p>Let's do an example in action.</p> <p>The following picture shows the example in the relevant paper, where we demonstrate causal discovery on a clique structure (e.g. all the variables in a clique are connected in pairs).</p> <p>A toy graphical structure, namely the clique = \\(\\{C, E, U\\}\\), illustrates how to determine non-linear identifiability under latent confounding (\\(\\overline{pa}_C\\) or \\(\\overline{pa}_E\\)) in cases (a), (b), and (c).</p>"},{"location":"quick_tutorials/#311-causal-sufficiency","title":"3.1.1 Causal Sufficiency","text":"<p>The causal sufficiency, namely considering the data generation without latent confounding.</p> <p>Let's take a quick look for the following code ignoring some details in the paper. For example, we simulate the data generation in figure (a) without (the latent confounder) \\(\\overline{pa}_C\\). <pre><code>import numpy as np\n\n# Fine-tune the data generation by scaling.\ndef scaling(variable, scale):\n    return variable * scale\n\nsample = 2000\n\nnp.random.seed(42)\n\n# Simulate the data generation with non-linear functional relations.\nc = np.random.normal(size=sample) \nu = scaling(np.cos(c), 1) + scaling(np.random.normal(size=sample), 0.1)\ne = scaling(np.sin(c), 1) + np.sin(u) + scaling(np.random.normal(size=sample), 0.1)\n\ndataset_a_without_confounding = np.array([c, e, u]).T\n</code></pre> The dataset entails non-linear causal relations,  so that we could try <code>NonlinearMLC</code> on it. <pre><code>nonlinear_mlc.fit(dataset=dataset_a_without_confounding)\n</code></pre> The following shows the visualization after applying <code>NonlinearMLC</code>. <pre><code>array = nonlinear_mlc.adjacency_matrix_\ndraw_graph_from_ndarray(\n    array=array, \n    # Rename the graph nodes to consist with the data column.\n    rename_nodes=['C', 'E', 'U']   \n)\nplt.show()\n</code></pre></p> <p><code>NonlinearMLC</code> uncovers the non-linear causal relation from the data! The figure illustrates a directed acyclic graph (DAG) representing the causal graph.</p>"},{"location":"quick_tutorials/#312-latent-confounders","title":"3.1.2 Latent Confounders","text":"<p>Next, let's do a bit of modification for the above data generation code.  In other words,  we simulate the data generation in figure (a) with the latent confounder \\(\\overline{pa}_C\\).</p> <p>Causal discovery with latent confounders</p> <p>CADIMULC features the algorithm that can handle multiple latent confounders.  For simplify, our demonstration here only describes the cases with a single latent confounder.</p> <p><pre><code>np.random.seed(42)\n\n# Add an unobserved parent variable. \npa_c = np.random.normal(size=sample)\n\n# Simulate latent confounding to the data generation.\nc = np.random.normal(size=sample) + scaling(pa_c, 0.5)\nu = scaling(np.cos(c), 1) + scaling(np.random.normal(size=sample), 0.1) + scaling(pa_c, 0.5)\ne = scaling(np.sin(c), 1) + np.sin(u) + scaling(np.random.normal(size=sample), 0.1)\n\ndataset_a_with_confounding = np.array([c, e, u]).T\n</code></pre> Conduct <code>NonlinearMLC</code> again, and visualize the causal graph we have discovered. <pre><code>nonlinear_mlc.fit(dataset=dataset_a_with_confounding)\n\ndraw_graph_from_ndarray(\n    array=nonlinear_mlc.adjacency_matrix_, \n    rename_nodes=['C', 'E', 'U']   \n)\nplt.show()\n</code></pre></p> <p>Notice that this time we get a partial directed acyclic graph as the output of <code>NonlinearMLC</code>. That is, the fact, where the bi-directed edge \\(U \\leftrightarrow C\\) that cannot be determined by <code>NonlinearMLC</code>, suggests the existence of latent confounder.</p> <p>Remarkably, the edge \\(C \\rightarrow E\\) (cause -&gt; effect) is still identifiable,  even if the indirected latent confounding \\(C \\leftarrow \\overline{pa}_C \\rightarrow U \\rightarrow E\\) persists between \\(C\\) and \\(E\\).</p> <p>The edge \\(C \\rightarrow E\\) (cause -&gt; effect) retains the identifiability as well when it comes to the simulation in figure (b).</p> <pre><code>np.random.seed(42)\n\npa_e = np.random.normal(size=sample) \n\nc = np.random.normal(size=sample) \nu = scaling(np.cos(c), 10) + scaling(np.random.normal(size=sample), 0.1) + scaling(pa_e, 0.5)\ne = scaling(np.sin(c), 10) + np.sin(u) + scaling(np.random.normal(size=sample), 0.1) + scaling(pa_e, 10)\n\ndataset_b_with_confounding = np.array([c, e, u]).T\n\nnonlinear_mlc.fit(dataset=dataset_b_with_confounding)\n\ndraw_graph_from_ndarray(\n    array=nonlinear_mlc.adjacency_matrix_, \n    rename_nodes=['C', 'E', 'U']   \n)\nplt.show()\n</code></pre> <p>Fine-tune the generation setting</p> <p>We fine-tune the scale of the \"causal strength\" among variables.  Technical procedures (such as independence tests and regression) involved in hybrid-based causal discovery algorithms are relatively sensitive to the empirical data. See more in Important Usage Advice at the bottom of this page.</p> <p>Notice that the result shows an unfaithful fact: the edge between \\(C\\) and \\(U\\) does not be learned by the algorithm. This is excusable, as it is known that the causal assumption  in constraint-based methods such as causal faithfulness is untestable in practical.</p> <p>Of course, here we could not discuss a lot about it in this short tutorial. </p> <p>The primary observation is that  the edge \\(C \\rightarrow E\\) keeps identifiable in figure (b) under the latent confounding resulted from \\(\\overline{pa}_E\\).</p> <p>Conversely, we will get a different result for another simulation in figure (c). <pre><code>np.random.seed(42)\n\npa_e = np.random.normal(size=sample) \n\nu = scaling(np.random.normal(size=sample), 0.1) + scaling(pa_e, 0.5)\nc = np.random.normal(size=sample) + scaling(np.cos(u), 10)\ne = scaling(np.sin(c), 10) + np.sin(u) + scaling(np.random.normal(size=sample), 0.1) + scaling(pa_e, 10)\n\ndataset_c_with_confounding = np.array([c, e, u]).T\n\nnonlinear_mlc.fit(dataset=dataset_c_with_confounding)\n\ndraw_graph_from_ndarray(\n    array=nonlinear_mlc.adjacency_matrix_, \n    rename_nodes=['C', 'E', 'U']   \n)\nplt.show()\n</code></pre></p> <p><code>NonlinearMLC</code> outputs an undirected graph (though the edge between \\(C\\) and \\(U\\) should have been identified).  But the point is, the edge \\(C \\rightarrow E\\) , unfortunately, becomes unidentifiable if  the latent confounding occurs in the way of figure (c).</p> <p> For brief summary:  Being able to help you clearly differentiate the (causal graph) identifiable nonlinear-dataset from another, with the presence of latent confounding, is the essence as to the relevant paper and the algorithmic programming in CADIMULC. </p>"},{"location":"quick_tutorials/#33-structural-causal-models","title":"3.3 Structural Causal Models","text":"<p>Tutorials in 3.1 tell us that,  in order to discover a \"causal structure\" from data, we might first need to learn about how to generate the data. </p> <p>In causation, this refers to how the Nature generates the data based on a \"structural causal model (SCM)\". You could use the <code>Generator</code> module in CADIMULC to learn  how to uniformly simulate the empirical data that entails causation. <pre><code>from cadimulc.utils.generation import Generator\n</code></pre> For example,  let us simulate an empirical dataset consisting of 3 variables (<code>graph_node_num=3</code>), with the  sample size equal to 1000 (<code>sample=2000</code>). <pre><code># CADIMULC uses the two following libraries for random graph simulation.\nimport numpy.random\nimport random\n\n# Simultaneously fix them before you would like to test the Generator.\nnumpy.random.seed(42)\nrandom.seed(42)\n\n# Default simulation setting as to a \"learnable\" SCM.\ngenerator = Generator(\n        graph_node_num=3,  \n        sample=2000,\n        causal_model='hybrid_nonlinear',\n        noise_type='Gaussian'\n    )\n\n# Simulation results: truth causal structure and empirical dataset.\nground_truth, dataset = generator.run_generation_procedure().unpack()\n</code></pre> The other parameters, such as <code>causal_model</code> and <code>noise_type</code>, specify a \"learnable\" SCM, which refers to the special additive models compared with the general SCMs spectrum. See SCMs Data Generation for a brief introduction.</p> <p>Now we could similarly apply the hybrid algorithms on the dataset we generate. We also have the <code>ground_truth</code> as the causal structure relative to the dataset.</p>"},{"location":"quick_tutorials/#4-evaluation","title":"4. Evaluation","text":"<p>If we have the ground-truth with respect to the causal relations we are interested in, then we can evaluate the estimated causal graph by using the evaluation module in CADIMULC.</p> <p><pre><code>from cadimulc.utils.evaluation import Evaluator\n</code></pre> By simply inputting two of the adjacency matrices representing the ground-truth and the estimated causal graph respectively, <code>Evaluator</code> will calculate the metrics relative to causal graphs, akin to the common indicators used in machine learning, such as causal edge precision, causal edge recall, and causal edge f1-score. <pre><code>causal_edge_precision = Evaluator.precision_pairwise(\n        true_graph=ground_truth,\n        est_graph=nonlinear_mlc.adjacency_matrix_\n)\n\ncausal_edge_recall = Evaluator.recall_pairwise(\n        true_graph=ground_truth,\n        est_graph=nonlinear_mlc.adjacency_matrix_\n)\n\ncausal_edge_f1_score = Evaluator.f1_score_pairwise(\n        true_graph=ground_truth,\n        est_graph=nonlinear_mlc.adjacency_matrix_\n)\n\nprint(causal_edge_precision, causal_edge_recall, causal_edge_f1_score)\n</code></pre> To see more usages and details, please refer to Evaluation and Visualization</p>"},{"location":"quick_tutorials/#5-important-usage-advice","title":"5. Important Usage Advice","text":"<p> Notice that the \"micro\" causal discovery workflow is only for the purpose of demonstration.  Technically speaking, mainstream causal discovery algorithms are \"noise-driven\" algorithms,  thus the performance of the algorithm is usually susceptible to the parameters of data generation.  <p>Development Testing and  the relevant paper  also show that data simulation in <code>Generator</code> might be over-rigorous that few causal algorithms perform well on that simulation dataset.</p> <p>Yet if developers would like to refer to this workflow, the following advice might be helpful:</p> <ul> <li> <p>Fine-tune the built-in simulation function in the source code of <code>Generator</code>, so that it fits the best with your domain knowledge. Simulation in CADIMULC wishes to provide you a causation generation framework, NOT a causation generation standard;</p> </li> <li> <p>Filter out some \"unlearnable\" generation dataset by running a series of random seed;</p> </li> <li> <p>Make sure you have involved other causal discovery methods as the baseline. </p> </li> </ul>"}]}