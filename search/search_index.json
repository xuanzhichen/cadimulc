{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>CADIMULC is a Python package standing: CAusal DIscovery  with Multiple Latent Confounders, providing easy-to-use light APIs  to learn an empirical causal graph from generally raw data with relatively efficiency. It integrates implementations of hybrid-based approaches involving the popular MLC-LiNGAM algorithm, along with the \"micro\" workflow of causal discovery, such as data generation, learning results evaluation, and graphs visualization.</p>"},{"location":"#who-we-are","title":"Who We Are?","text":"<p>The hybrid methodology built in CADIMULC refers to the causal discovery framework that was early proposed and developed by the  Data Mining and Information Retrieval laboratory (DMIR lab, PI: Ruichu Cai). The lab has also been collaboratively developing causal-learn (Python package for causal discovery) by CMU.</p> <p>Xuanzhi Chen is currently the owner of the repository, but maintenance and updates might not be timely since CADIMULC is limited in personally developed.  Xuanzhi Chen is sorry about that, but efforts of opening the issue and  advancing the community are always welcomed. (Reach Out: xuanzhichen.42@gmail.com)</p>"},{"location":"#overview","title":"Overview","text":""},{"location":"#what-is-the-hybrid-based-approach-in-causal-discovery","title":"What Is the Hybrid-Based Approach in Causal Discovery?","text":"<p>In fields of causal discovery,  conditional-independence-test (CIT) based methods and functional-causal-models (FCMs) based methods are the dominantly popular methodology over the last two decades. Representative hybrid-based causal discovery algorithms, such as  SADA [1] and MLC-LiNGAM [2], are meant to hybridize the advantages of the two theory.</p> <p>The following picture shows the theory blueprint for the algorithmic programming:</p> <p>For details, please refer to the main idea  in CADIMULC's related paper, Section 4.</p> <ol> <li>resort to CIT approaches to construct network skeleton; </li> <li>apply FCMs approaches based on the well-known divide-and-conquer strategy.</li> </ol>"},{"location":"#the-defining-feature-of-cadimulc","title":"The Defining Feature of Cadimulc","text":"<p>Equipped with the repertoire of hybrid-based approaches,  CADIMULC further focuses on the algorithmic efficiency and robustness in general causal discovery. Namely, relatively relax causal assumptions, such as causal sufficiency and linearity, upon the data. For example:</p> <ul> <li>given common disturbance from the multiple unknown factor;</li> <li>given the \"generic\" non-linear relation entailed by data.</li> </ul> <p>Causal discovery by CADIMULC is still anticipated to be relatively scalable in computation and reliable in performance.</p>"},{"location":"#modules-for-the-micro-workflow-of-causal-discovery","title":"Modules for the \"Micro\" Workflow of Causal Discovery","text":"<p>Write down the descriptions here.</p> Module Workflow Description utils.Generator Data generation test hybrid_algorithms.MLCLiNGAM Causal Discovery test hybrid_algorithms.NonlinearMLC Causal Discovery partial directed graph entailed by non-linear data utils.Evaluator Evaluation test utils.draw_graph_from_array Visualization test"},{"location":"#citation","title":"Citation","text":"<p>Please cite the following paper(s) depending on which approach you use in your reports or publications: <pre><code>@article{chen2021causal,\n  title={Causal discovery in linear non-gaussian acyclic model with multiple latent confounders},\n  author={Chen, Wei and Cai, Ruichu and Zhang, Kun and Hao, Zhifeng},\n  journal={IEEE Transactions on Neural Networks and Learning Systems},\n  volume={33},\n  number={7},\n  pages={2816--2827},\n  year={2021},\n  publisher={IEEE}\n}\n</code></pre></p>"},{"location":"#license","title":"License","text":"<p>Copyright (C) 2022-2024 Xuanzhi Chen (DMIR lab, Guangdong University of Technology, China)</p> <p>CADIMULC is downloaded for free, provided \"as is\" WITHOUT ANY EXPRESS OR IMPLIED WARRANTY; CADIMULC is developed in hope of being beneficial for empirical data analysis in causation, but WITHOUT WARRANTY OF ABSOLUTELY ACCURATE INTERPRETATION.</p>"},{"location":"#reference","title":"Reference","text":"<p>[1] Cai, Ruichu, Zhenjie Zhang, and Zhifeng Hao. Sada: A general framework to support robust causation discovery. International conference on machine learning, PMLR. 2013.</p> <p>[2] Chen, Wei, Ruichu Cai, Kun Zhang, and Zhifeng Hao. Causal discovery in linear non-gaussian acyclic model with multiple latent confounders.  IEEE Transactions on Neural Networks and Learning Systems. 2021.</p>"},{"location":"acknowledgements/","title":"Acknowledgements","text":"<p>Xuanzhi Chen would like to thank the DMIR laboratory for offering him research opportunities,  with special thanks to Ruichu Cai (\u8521\u745e\u521d) of the lab director and Dongning Liu (\u5218\u51ac\u5b81) of the dean in School of Computer.</p> <p>Jie Qiao (\u4e54\u6770) and Zhiyi Huang (\u9ec4\u667a\u6bc5) were willing to spend their time in personal discussions with Xuanzhi Chen  about details in the paper; Zhengming Chen (\u9648\u6b63\u94ed) patiently helped point out initial mistakes in the paper; thanks to other graduate students, such as Zeqin Yang (\u6768\u6cfd\u52e4), Xiaokai Huang (\u9ec4\u6653\u6977), and Yu Xiang (\u5411\u5b87), for their generosity of teaching when Xuanzhi chen was initially building the repository.</p> <p>Finally, Xuanzhi Chen owes a great debt to his advisor Wei Chen (\u9648\u8587) for her encouragement  when Xuanzhi started studying causation two years ago \u2014 \"Do it, just have your own interest of research and your own rhythm of lifetime\".</p>"},{"location":"evaluation/","title":"Evaluation and Visualization","text":"<p>Advice for developers if needed: Evaluation and visualization</p> <p>Algorithms in CADIMULC simply represent the causation among variables, for both ground-truth  and learning results, as the directed pairs in an adjacency matrix with only two elements 0 and 1.</p> <p>If you incline to this representation of data structure in your work or research,  then <code>Evaluator</code> in CADIMULC might provide you convenience for evaluating the causal graph directly.</p> Class: Evaluator <p>---</p> Primary Method: precision_pairwise <p>Causal pair precision refers to the proportion of the correctly estimated directed pairs in all the estimated directed pairs (EDP):</p> \\[     Precision = TP \\ / \\  (TP + FP) = TP \\ / \\ EDP. \\] <p>The higher the precision, the larger the amount of the causal pairs, compared to EDP, that are identified, without considering the amount of unestimated pairs.</p> <p>Parameters:</p> Name Type Description Default <code>true_graph</code> <code>ndarray</code> <p>True causal graph, namely the ground-truth.</p> required <code>est_graph</code> <code>ndarray</code> <p>Estimated causal graph, namely the empirical causal graph.</p> required <p>Returns:</p> Name Type Description <code>precision</code> <code>float</code> <p>Precision of the \"causal discovery task\".</p> Source code in <code>cadimulc\\utils\\evaluation.py</code> <pre><code>@staticmethod\ndef precision_pairwise(true_graph: ndarray, est_graph: ndarray) -&gt; float:\n    \"\"\"\n    **Causal pair precision** refers to the proportion of the correctly estimated\n    directed pairs in all the **estimated directed pairs** (EDP):\n\n    $$\n        Precision = TP \\ / \\  (TP + FP) = TP \\ / \\ EDP.\n    $$\n\n    The higher the precision, the larger the amount of the causal pairs,\n    compared to EDP, that are identified,\n    without considering the amount of **unestimated** pairs.\n\n    Parameters:\n        true_graph: True causal graph, namely the ground-truth.\n        est_graph: Estimated causal graph, namely the empirical causal graph.\n        learned from data.\n\n    Returns:\n        precision: Precision of the \"causal discovery task\".\n    \"\"\"\n\n    _, dict_directed_parent = Evaluator.get_pairwise_info(true_graph)\n    est_pairs = Evaluator.get_directed_pairs(est_graph)\n    num_est_pairs = len(est_pairs)\n\n    if num_est_pairs &gt; 0:\n        tp = 0\n\n        for est_pair in est_pairs:\n            child = est_pair[1]\n            parent = est_pair[0]\n\n            if parent in dict_directed_parent[child]:\n                tp += 1\n\n        precision = round((tp / num_est_pairs), 3)\n\n    else:\n        precision = float(0)\n\n    return precision\n</code></pre> Primary Method: recall_pairwise <p>Causal pair recall refers to the proportion of correctly estimated directed pairs in all true causal pairs (TCP):</p> \\[     Recall = TP \\ / \\  (TP + FN) = TP \\ / \\  TCP \\] <p>The higher the recall, the larger the amount of the causal pairs, compared to TCP, that are identified, without considering the amount of incorrectly estimated pairs.</p> <p>Parameters:</p> Name Type Description Default <code>true_graph</code> <code>ndarray</code> <p>True causal graph, namely the ground-truth.</p> required <code>est_graph</code> <code>ndarray</code> <p>Estimated causal graph, namely the empirical causal graph.</p> required <p>Returns:</p> Name Type Description <code>recall</code> <code>float</code> <p>Recall of the \"causal discovery task\".</p> Source code in <code>cadimulc\\utils\\evaluation.py</code> <pre><code>@staticmethod\ndef recall_pairwise(true_graph: ndarray, est_graph: ndarray) -&gt; float:\n    \"\"\"\n    **Causal pair recall** refers to the proportion of correctly estimated directed\n    pairs in all **true causal pairs** (TCP):\n\n    $$\n        Recall = TP \\ / \\  (TP + FN) = TP \\ / \\  TCP\n    $$\n\n    The higher the recall, the larger the amount of the causal pairs,\n    compared to TCP,\n    that are identified, without considering the amount of **incorrectly** estimated pairs.\n\n    Parameters:\n        true_graph: True causal graph, namely the ground-truth.\n        est_graph: Estimated causal graph, namely the empirical causal graph.\n\n    Returns:\n        recall: Recall of the \"causal discovery task\".\n    \"\"\"\n\n    num_directed_pairs, dict_directed_parent = (\n        Evaluator.get_pairwise_info(true_graph))\n\n    est_pairs = Evaluator.get_directed_pairs(est_graph)\n    num_est_pairs = len(est_pairs)\n\n    if num_directed_pairs == 0:\n        recall = float(1)\n\n    elif num_est_pairs == 0:\n        recall = float(0)\n\n    else:\n        tp = 0\n\n        for est_pair in est_pairs:\n            child = est_pair[1]\n            parent = est_pair[0]\n\n            if parent in dict_directed_parent[child]:\n                tp += 1\n\n        recall = round((tp / num_directed_pairs), 3)\n\n    return recall\n</code></pre> Primary Method: f1_score_pairwise <p>Causal pair F1-score, the concordant mean of the precision and recall, represents the global measurement of causal discovery, bring together the advantages from both the precision and recall.</p> \\[     F1 = (2 * Precision * Recall)\\  / \\ (Precision + Recall). \\] <p>Parameters:</p> Name Type Description Default <code>true_graph</code> <code>ndarray</code> <p>True causal graph, namely the ground-truth.</p> required <code>est_graph</code> <code>ndarray</code> <p>Estimated causal graph, namely the empirical causal graph.</p> required <p>Returns:</p> Name Type Description <code>f1_score</code> <code>float</code> <p>F1-score of the \"causal discovery task\".</p> Source code in <code>cadimulc\\utils\\evaluation.py</code> <pre><code>@staticmethod\ndef f1_score_pairwise(true_graph: ndarray, est_graph: ndarray) -&gt; float:\n    \"\"\"\n    **Causal pair F1-score**, the concordant mean of the precision and recall,\n    represents the global measurement of causal discovery, bring together the\n    advantages from both the precision and recall.\n\n    $$\n        F1 = (2 * Precision * Recall)\\  / \\ (Precision + Recall).\n    $$\n\n    Parameters:\n        true_graph: True causal graph, namely the ground-truth.\n        est_graph: Estimated causal graph, namely the empirical causal graph.\n\n    Returns:\n        f1_score: F1-score of the \"causal discovery task\".\n    \"\"\"\n\n    precision = Evaluator.precision_pairwise(true_graph, est_graph)\n    recall = Evaluator.recall_pairwise(true_graph, est_graph)\n\n    if (precision + recall) != 0:\n        f1_score = round(\n            (2 * precision * recall) / (precision + recall), 3\n        )\n\n    else:\n        f1_score = float(0)\n\n    return f1_score\n</code></pre> Primary Method: evaluate_skeleton <p>Note</p> <p>Construction of a network skeleton is the fundamental part relative to the procedure of hybrid-based approaches.  CADIMULC also provides simply way to evaluate the causal skeleton. Notice that performance of the hybrid-based approach largely depends on the initial performance of the causal skeleton learning.</p> <p>The <code>evaluate_skeleton</code> method evaluates a network skeleton based on an assigned metric. To this end, available metrics mirroring to the causal pair evaluation  are list as the following:</p> <ul> <li>Skeleton Precision = TP (of the estimated skeleton) / all estimated edges.</li> <li>Skeleton Recall = TP (of the estimated skeleton) / all true edges.</li> <li>Skeleton F1-score = (2 * Precision * Recall) / (Precision + Recall).</li> </ul> <p>Parameters:</p> Name Type Description Default <code>true_skeleton</code> <code>ndarray</code> <p>True causal skeleton, namely the ground-truth.</p> required <code>est_skeleton</code> <code>ndarray</code> <p>Estimated causal skeleton, namely the empirical causal skeleton.</p> required <code>metric</code> <code>str</code> <p>selective metrics from <code>['Precision', 'Recall', or 'F1-score']</code>.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The evaluating value of the causal skeleton in light of the assigned metric.</p> Source code in <code>cadimulc\\utils\\evaluation.py</code> <pre><code>@staticmethod\ndef evaluate_skeleton(\n        true_skeleton: ndarray,\n        est_skeleton: ndarray,\n        metric: str\n) -&gt; float:\n    \"\"\"\n    The `evaluate_skeleton` method evaluates a network skeleton based on an assigned\n    metric. To this end, available metrics mirroring to the causal pair evaluation\n     are list as the following:\n\n    * **Skeleton Precision** = TP (of the estimated skeleton) / all estimated edges.\n    * **Skeleton Recall** = TP (of the estimated skeleton) / all true edges.\n    * **Skeleton F1-score** = (2 * Precision * Recall) / (Precision + Recall).\n\n    Parameters:\n        true_skeleton: True causal skeleton, namely the ground-truth.\n        est_skeleton: Estimated causal skeleton, namely the empirical causal skeleton.\n        metric: selective metrics from `['Precision', 'Recall', or 'F1-score']`.\n\n    Returns:\n        The evaluating value of the causal skeleton in light of the assigned metric.\n    \"\"\"\n\n    true_skeleton_nx = nx.from_numpy_array(true_skeleton)\n    est_skeleton_nx = nx.from_numpy_array(est_skeleton)\n\n    true_edges = list(true_skeleton_nx.edges())\n    est_edges = list(est_skeleton_nx.edges())\n\n    tp_skeleton = 0\n    for est_edge in est_edges:\n        if est_edge in true_edges:\n            tp_skeleton += 1\n\n    if len(est_edges) &gt; 0:\n        precision = round(tp_skeleton / len(est_edges), 3)\n    else:\n        precision = float(0)\n\n    if len(true_edges) &gt; 0:\n        recall = round(tp_skeleton / len(true_edges), 3)\n    else:\n        recall = float(0)\n\n    if precision + recall &gt; 0:\n        f1_score = round(\n            2 * ((precision * recall) / (precision + recall)), 3\n        )\n    else:\n        f1_score = float(0)\n\n    if metric == 'Precision':\n        return precision\n\n    elif metric == 'Recall':\n        return recall\n\n    elif metric == 'F1-score':\n        return f1_score\n\n    else:\n        raise ValueError(\"Please input established metric types: \"\n                         \"'Precision', 'Recall', or 'F1-score'.\")\n</code></pre> Secondary Method: get_directed_pairs <p>Extract directed pairs from a graph.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>ndarray</code> <p>An adjacency bool matrix representing the causation among variables.</p> required <p>Returns:</p> Name Type Description <code>direct_pairs</code> <code>list[list]</code> <p>A list whose elements are in form of [parent, child], referring to the causation parent -&gt; child.</p> Source code in <code>cadimulc\\utils\\evaluation.py</code> <pre><code>@staticmethod\ndef get_directed_pairs(graph: ndarray) -&gt; list[list]:\n    \"\"\"\n    Extract directed pairs from a graph.\n\n    Parameters:\n        graph: An adjacency bool matrix representing the causation among variables.\n\n    Returns:\n        direct_pairs:\n            A list whose elements are in form of [parent, child], referring to the\n            causation parent -&gt; child.\n    \"\"\"\n\n    dim = graph.shape[0]\n    directed_pairs = []\n\n    for j in range(dim):\n        for i in range(dim):\n            if graph[i][j] == 1 and graph[j][i] == 0:\n                directed_pairs.append([j, i])\n\n    return directed_pairs\n</code></pre> Secondary Method: get_pairwise_info <p>Obtain information related to a given directed graph: (1) number of the directed pairs; (2) parents-child pairing relationships.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>ndarray</code> <p>An adjacency bool matrix representing the causation among variables.</p> required <p>Returns:</p> Type Description <code>(int, dict)</code> <p><code>num_directed_pairs</code> as the number of directed pairs and <code>directed_parent_dict</code> as the dictionary representing the parent-child pairing relationships.</p> Source code in <code>cadimulc\\utils\\evaluation.py</code> <pre><code>@staticmethod\ndef get_pairwise_info(graph: ndarray) -&gt; (int, dict):\n    \"\"\"\n    Obtain information related to a given directed graph:\n    (1) number of the directed pairs; (2) parents-child pairing relationships.\n\n    Parameters:\n        graph: An adjacency bool matrix representing the causation among variables.\n\n    Returns:\n        `num_directed_pairs` as the number of directed pairs and `directed_parent_dict`\n         as the dictionary representing the parent-child pairing relationships.\n    \"\"\"\n\n    directed_parent_dict = {}\n    num_directed_pairs = 0\n    dim = graph.shape[0]\n\n    for i in range(dim):\n        directed_parent_dict[i] = set()\n\n    for i in range(dim):\n        for j in range(dim):\n            if graph[i][j] == 1:\n                directed_parent_dict[i].add(j)\n                num_directed_pairs += 1\n\n    return num_directed_pairs, directed_parent_dict\n</code></pre> <p>examples for these methods</p> <p>CADIMULC is a light Python repository without sophisticated library API design. Documentation on this page is meant to provide introductory materials of the practical tool as to causal discovery. For running example,  please simply check out Quick Tutorials for the straightforward usage in the \"micro\" workflow of  causal discovery.</p>"},{"location":"evaluation/#cadimulc.utils.evaluation.Evaluator","title":"<code>cadimulc.utils.evaluation.Evaluator</code>","text":"<p>Given an instance as to causal discovery, the <code>Evaluator</code> defines the classification errors between an actual graph and a predicted graph, which is corresponding to, in the field of machine learning, the four of the categories within a confusion matrix.</p> <ul> <li> <p>TP (True Positives): The number of the estimated directed pairs that   are consistent with the true causal pairs. Namely, TP qualifies the   correct estimation of causal relations.</p> </li> <li> <p>FP (False Positives): The number of the estimated directed pairs that   do not present in the true causal pairs.</p> </li> <li> <p>TN: (True Negatives): The number of the unestimated directed pairs   that are consistent with the true causal pairs. TN reflects   the correct prediction of unpresented causal relations.</p> </li> <li> <p>FN (False Negatives): The number of the unestimated directed pairs   that do present in the true causal pairs.</p> </li> </ul> <p>The only assessment of directed causal relations</p> <p>The <code>Evaluator</code> focuses on the assessment of estimated directed pairs (TP) extracted from an adjacency matrix, treating the rest as unpresented pairs (FP) relative to the ground-truth.</p> <p>In other words, <code>Evaluator</code> in CADIMULC does not explicitly consider bi-directed pairs or undirected pairs.</p>"},{"location":"generation/","title":"SCMs Data Generation","text":"<p>Advice for developers if needed: SCMs data generation</p> <p><code>Generator</code> in CADIMULC serves as a framework of general data generation in  the task of causal discovery.  Default settings of hyperparameters (e.g. parameters of specific causal function)  in the <code>Generator</code> might require being fine-tuned depends on different purposes for simulation.</p> <p>Users could develop their own \"causal simulator\" in data analysis based on their need  interest, by following the data generation template in <code>Generator</code>.</p> Class: Generator Primary Method: run_generation_procedure <p>Run the common two-steps procedure for SCMs data generation:</p> <ol> <li>Generate a random DAG in light of the well-known Erd\u0151s\u2013R\u00e9nyi model;</li> <li>Provided a topological order converted by the DAG, generate each variable \\(y_i\\) by summarizing the effects of its parents \\(pa(y_i)\\).</li> </ol> <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Update the <code>Generator</code>'s attributes: <code>_skeleton</code> as the undirected graph corresponding to the causal graph (ground-truth), <code>_dag</code> as the directed acyclic graph (DAG) corresponding to the (ground-truth), and <code>_dataset</code> in a format (n * d) (n = <code>sample</code>, d = <code>graph_node_num</code>).</p> Source code in <code>cadimulc\\utils\\generation.py</code> <pre><code>def run_generation_procedure(self) -&gt; object:\n    \"\"\" Run the common **two-steps** procedure for SCMs data generation:\n\n    1. Generate a random DAG in light of the well-known Erd\u0151s\u2013R\u00e9nyi model;\n    2. Provided a topological order converted by the DAG, generate each variable $y_i$ by\n    summarizing the effects of its parents $pa(y_i)$.\n\n    Returns:\n        self:\n            Update the `Generator`'s attributes: `_skeleton` as the undirected graph\n            corresponding to the causal graph (ground-truth), `_dag` as the\n            directed acyclic graph (DAG) corresponding to the (ground-truth),\n            and `_dataset` in a format (n * d) (n = `sample`, d = `graph_node_num`).\n    \"\"\"\n\n    self._clear()\n\n    # Generate a random DAG in light of the well-known Erd\u0151s\u2013R\u00e9nyi model.\n    self._generate_dag()\n\n    # Provided a topological order converted by the DAG, generate each variable by\n    # summarizing the effects of its parents\n    self._generate_data()\n\n    return self\n</code></pre> Private Method: _generate_dag <p>Generate a random DAG in light of the well-known Erd\u0151s\u2013R\u00e9nyi model.</p> <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Update <code>_dag</code> (DAG) represented as a bool adjacency matrix.</p> Source code in <code>cadimulc\\utils\\generation.py</code> <pre><code>def _generate_dag(self) -&gt; object:\n    \"\"\"\n    Generate a random DAG in light of the well-known Erd\u0151s\u2013R\u00e9nyi model.\n\n    Returns:\n        self: Update `_dag` (DAG) represented as a bool adjacency matrix.\n    \"\"\"\n\n    # undirected graph\n    undigraph = self._get_undigraph(\n        graph_node_num=self.graph_node_num,\n        sparsity=self.sparsity\n    )\n    self.skeleton = copy(undigraph)\n\n    # directed graph\n    digraph = self._orient(undigraph)\n    self.dag = copy(digraph)\n\n    return self\n</code></pre> Private Method: _generate_data <p>Provided a topological order converted by the DAG, generate each variable by summarizing the effects of its parents.</p> <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Update <code>_data</code> represented as a (n * d) numpy array (n = sample, d = graph_node_num).</p> Source code in <code>cadimulc\\utils\\generation.py</code> <pre><code>def _generate_data(self) -&gt; object:\n    \"\"\"\n    Provided a topological order converted by the DAG, generate each variable by\n    summarizing the effects of its parents.\n\n    Returns:\n        self: Update `_data` represented as a (n * d) numpy array (n = sample,\n         d = graph_node_num).\n    \"\"\"\n\n    self.data = np.zeros([self.sample, self.graph_node_num])\n\n    # topological order converted by the DAG\n    dag_nx = nx.DiGraph(self.dag.T)\n    topo_order = list(nx.topological_sort(dag_nx))\n\n    # data generation in light of additive noise model\n    for child_index, child_var in enumerate(topo_order):\n        parent_vars = list(dag_nx.predecessors(child_var))\n        parent_indexes = [topo_order.index(var) for var in parent_vars]\n\n        # independence noise\n        self._add_random_noise(var_index=child_index)\n\n        # summary of the parents effects\n        if len(parent_vars) &gt; 0:\n            for parent_index in parent_indexes:\n                self._simulate_causal_model(\n                    child_index=child_index,\n                    parent_index=parent_index\n                )\n\n    # standard deviation of the dataset (default setting)\n    self.data = self.data / np.std(self.data, axis=0)\n\n    return self\n</code></pre> <p>examples for these methods</p> <p>CADIMULC is a light Python repository without sophisticated library API design. Documentation on this page is meant to provide introductory materials of the practical tool as to causal discovery. For running example,  please simply check out Quick Tutorials for the straightforward usage in the \"micro\" workflow of  causal discovery.</p>"},{"location":"generation/#cadimulc.utils.generation.Generator","title":"<code>cadimulc.utils.generation.Generator</code>","text":"<p>             Bases: <code>object</code></p> <p>The <code>Generator</code> simulates the empirical data implied by the structure causal models (SCMs). Primary parameters for <code>Generator</code>'s simulation consist of the model classes (e.g. linear or non-linear) and the (independent) noise distributions (e.g. Gaussian or non-Gaussian).</p> <p>Take causation in graphical context, where a variable \\(y_i\\) is supposed to be the effect of its parents \\(pa(y_i)\\). Then the data relative to \\(y_i\\) is expected to be generated given the (group of) data relative to \\(pa(y_i)\\), following the causal mapping mechanism \\(F\\) characterized as SCMs.</p> <p>Currently, research in causal discovery has suggested that structural-identifiable empirical data should be further generated by a special \"genus\" of the SCMs, which is normally referred to as the additive noise models (ANMs) shown in the following</p> \\[ y_{i} := F(pa(y_i), e_{i}):= \\sum_{x_{j} \\in pa(y_i)} f( x_{j}) + e_{i}, \\] <p>where \\(f(\\cdot)\\) denotes the linear or non-linear function, and \\(e_{i}\\) refers to the independent noise obeying the Gaussian or non-Gaussian distributions.</p> <p>Structural-identifiable SCMs simulation in CADIMULC in light of related literature</p> <p>linear: linear non-Gaussian acyclic models (LiNGAM)<sup>[1]</sup>, referring to the experiment setup by MLC-LiNGAM<sup>[2]</sup>.</p> <p>non-linear: causal additive models (CAM)<sup>[3]</sup>, referring to the experiment setup by CAM-UV<sup>[4]</sup>.</p>"},{"location":"generation/#cadimulc.utils.generation.Generator.__init__","title":"<code>__init__(graph_node_num, sample, causal_model='hybrid_nonlinear', noise_type='Gaussian', noise_scale='default', sparsity=0.3, _skeleton=None, _dag=None, _dataset=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>graph_node_num</code> <code>int</code> <p>Number of the vertex in a causal graph (ground-truth), which represents the number of the variable given a causal model (recommend: &lt; 15).</p> required <code>sample</code> <code>int</code> <p>Size of the dataset generated from the SCMs (recommend: &lt; 10000).</p> required <code>causal_model</code> <code>str</code> <p>Refer to structural-identifiable SCMs simulation in light of related literature. e.g. LiNGAM (str: lingam), CAM (str: hybrid_nonlinear).</p> <code>'hybrid_nonlinear'</code> <code>noise_type</code> <code>str</code> <p>Refer to structural-identifiable SCMs simulation in light of related literature. e.g. Gaussian (str: Gaussian), uniform distribution as non-Gaussian (str: non-Gaussian).</p> <code>'Gaussian'</code> <code>noise_scale</code> <code>int | str</code> <p>\"Default\" as following the experiment setup in light of related literature.</p> <code>'default'</code> <code>sparsity</code> <code>float</code> <p>Control the sparsity of a causal graph (ground-truth) (recommend: 0.3).</p> <code>0.3</code> <p>The causal model should be carefully paired with the noise type</p> <ul> <li>If <code>causal model = \"lingam\"</code>, the noise distribution     must satisfy <code>noise_type=\"non-Gaussian\"</code>.</li> <li>If <code>causal model = \"hybrid_nonlinear\"</code>, the noise distribution     can either choose <code>noise_type=\"Gaussian\"</code> or <code>noise_type=\"non-Gaussian\"</code>.     However, evaluation in CADIMULC suggests that Gaussian noise is more preferable     to yield identifiable results.</li> </ul>"},{"location":"generation/#reference","title":"Reference","text":"<p>[1] Shimizu, Shohei, Patrik O. Hoyer, Aapo Hyv\u00e4rinen, Antti Kerminen, and Michael Jordan.  \"A linear non-Gaussian acyclic model for causal discovery.\"  Journal of Machine Learning Research. 2006.</p> <p>[2] Chen, Wei, Ruichu Cai, Kun Zhang, and Zhifeng Hao. \"Causal discovery in linear non-gaussian acyclic model with multiple latent confounders. \" IEEE Transactions on Neural Networks and Learning Systems. 2021.</p> <p>[3] B\u00fchlmann, Peter, Jonas Peters, and Jan Ernest.  \"CAM: Causal additive models, high-dimensional order search and penalized regression.\"  2014. </p> <p>[4] Maeda, Takashi Nicholas, and Shohei Shimizu.  \"Causal additive models with unobserved variables.\"  In Uncertainty in Artificial Intelligence. 2021.</p>"},{"location":"hybrid_algorithms/","title":"Hybrid-Based Approaches","text":"Class: MLC-LiNGAM <sup>[1]</sup> <p>The output corresponding to the MLC-LiNGAM algorithm</p> <p>In the field of causal discovery, causal graphs are usually represented as the directed acyclic graph (DAG) or the causal order.  The existence of latent confounders, however, might result in the causal relations that cannot be determined by algorithms. </p> <p>Correspondingly, the estimated causal graph, by the MLC-LiNGAM or Nonlinear-MLC algorithm in CADIMULC, is represented as the partial directed acyclic graph or  the partial causal order.</p> Primary Method: fit <p>Fitting data via the MLC-LiNGAM causal discovery algorithm:</p> <ul> <li>Stage-I: Utilize the constraint-based method to learn a causal skeleton.</li> <li>Stage-II: Identify the causal directions by conducting regression and independence tests on the adjacent pairs in the causal skeleton.</li> <li>Stage-III: Detect the latent confounders with the help of the maximal clique patterns raised by the latent confounders, and reconstruct the causal structure with latent variables.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>ndarray</code> <p>The observational dataset shown as a matrix or table, with a format of \"sample (n) * dimension (d).\" (input as Pandas dataframe is also acceptable)</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Update the <code>adjacency_matrix</code> represented as an estimated causal graph. The <code>adjacency_matrix</code> is a (d * d) numpy array with 0/1 elements characterizing the causal direction.</p> Source code in <code>cadimulc\\hybrid_algorithms\\hybrid_algorithms.py</code> <pre><code>def fit(self, dataset: ndarray) -&gt; object:\n    \"\"\"\n    Fitting data via the *MLC-LiNGAM* causal discovery algorithm:\n\n    - **Stage-I**: Utilize the constraint-based method to learn a **causal skeleton**.\n    - **Stage-II**: Identify the causal directions by conducting **regression** and **independence tests**\n    on the adjacent pairs in the causal skeleton.\n    - **Stage-III**: Detect the latent confounders with the help of the **maximal clique patterns**\n    raised by the latent confounders,\n    and reconstruct the causal structure with latent variables.\n\n    Parameters:\n        dataset:\n            The observational dataset shown as a matrix or table,\n            with a format of \"sample (n) * dimension (d).\"\n            (input as Pandas dataframe is also acceptable)\n\n    Returns:\n        self:\n            Update the ``adjacency_matrix`` represented as an estimated causal graph.\n            The ``adjacency_matrix`` is a (d * d) numpy array with 0/1 elements\n            characterizing the causal direction.\n    \"\"\"\n\n    # stage-1: causal skeleton reconstruction(PC-stable algorithm)\n    self._stage_1_learning(dataset)\n\n    graph_pattern_manager = GraphPatternManager(init_graph=self._skeleton)\n\n    # stage-2: partial causal orders identification\n    self._stage_2_learning(graph_pattern_manager)\n\n    graph_pattern_manager.store_last_managing_adjacency_matrix()\n\n    # stage-3: latent confounders' detection\n    self._stage_3_learning(graph_pattern_manager)\n\n    return self\n</code></pre> Private Method: _stage_1_learning <p>Stage-I: Causal skeleton construction (based on the PC-stable algorithm).</p> <p>Stage-I begins with a complete undirected graph and performs conditional independence tests to delete the edges between independent variables pairs, reducing the computational cost of subsequent regressions and independence tests.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>ndarray</code> <p>The observational dataset shown as a matrix or table, with a format of \"sample (n) * dimension (d).\" (input as Pandas dataframe is also acceptable)</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Update <code>_skeleton</code> as the estimated undirected graph corresponding to the causal graph, initialize <code>_adjacency_matrix</code> via a copy of <code>_skeleton</code>, and record <code>_stage1_time</code> as the stage-1 computational time.</p> Source code in <code>cadimulc\\hybrid_algorithms\\hybrid_algorithms.py</code> <pre><code>def _stage_1_learning(self, dataset: ndarray) -&gt; object:\n    \"\"\"\n    **Stage-I**: Causal skeleton construction (based on the PC-stable algorithm).\n\n    Stage-I begins with a complete undirected graph and performs **conditional\n    independence tests** to delete the edges between independent variables pairs,\n    reducing the computational cost of subsequent regressions and independence tests.\n\n    Parameters:\n        dataset:\n            The observational dataset shown as a matrix or table,\n            with a format of \"sample (n) * dimension (d).\"\n            (input as Pandas dataframe is also acceptable)\n\n    Returns:\n        self:\n            Update `_skeleton` as the estimated undirected graph corresponding to\n            the causal graph, initialize `_adjacency_matrix` via a copy of `_skeleton`,\n            and record `_stage1_time` as the stage-1 computational time.\n    \"\"\"\n\n    self._causal_skeleton_learning(dataset)\n\n    return self\n</code></pre> Private Method: _stage_2_learning <p>Stage-II: Partial causal order identification.</p> <p>Based on the causal skeleton obtained by stage-I, stage II in MLC-LiNGAM  aims to determine the causal directions, identifying the observed variables that are not affected by latent confounders, and the partial causal orders among the adjacent observed variables in the causal skeleton.</p> <p>Parameters:</p> Name Type Description Default <code>graph_pattern_manager</code> <p>An auxiliary module embedded in the MLC-LiNGAM algorithm, featuring the algorithmic behavior of the maximal-cliques pattern recognition.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Update <code>_adjacency_matrix</code> as the estimated (partial) directed acyclic graph (DAG) corresponding to the causal graph, and <code>record _stage2_time</code> as the <code>stage-2</code> computational time.</p> Source code in <code>cadimulc\\hybrid_algorithms\\hybrid_algorithms.py</code> <pre><code>def _stage_2_learning(self, graph_pattern_manager) -&gt; object:\n    \"\"\"\n    **Stage-II**: Partial causal order identification.\n\n    Based on the causal skeleton obtained by stage-I,\n    stage II in *MLC-LiNGAM*  aims to determine the causal directions,\n    identifying the observed variables that are not affected by\n    latent confounders, and the partial causal orders among\n    the adjacent observed variables in the causal skeleton.\n\n    Parameters:\n        graph_pattern_manager:\n            An auxiliary module embedded in the MLC-LiNGAM algorithm,\n            featuring the algorithmic behavior of the maximal-cliques pattern recognition.\n\n    Returns:\n        self:\n            Update `_adjacency_matrix` as the estimated (partial) directed acyclic\n            graph (DAG) corresponding to the causal graph,\n            and `record _stage2_time` as the `stage-2` computational time.\n    \"\"\"\n\n    # Arguments for testing:\n    #   _skeleton(attribute): ndarray\n    #   _dataset(attribute): ndarray\n    #   _dim(attribute): int\n\n    start = time.perf_counter()\n\n    # Reconstruction of the causal skeleton entails specific pairs of adjacent variables,\n    # rather than all pairs of variables.\n    causal_skeleton = self._skeleton\n\n    # MLC-LiNGAM performs regression and independence tests efficiently\n    # based on the adjacency set.\n    adjacent_set = GraphPatternManager.find_adjacent_set(\n        causal_skeleton=causal_skeleton\n    )\n\n    # Apply Algorithm-2 (given by the MLC-LiNGAM algorithm).\n    self._algorithm_2(\n        corresponding_adjacent_set=adjacent_set,\n        corresponding_dataset=cp.copy(self._dataset),\n        corresponding_variables=np.arange(self._dim),\n        graph_pattern_manager=graph_pattern_manager\n    )\n\n    # Record computational time.\n    end = time.perf_counter()\n    self._stage2_time = end - start\n\n    return self\n</code></pre> Private Method: _stage_3_learning <p>Stage-III: Latent confounders' detection</p> <p>Stage-III will detect whether the remaining variables share the same latent confounders. Specifically, the stage-III learning makes use of the causal skeleton information to reduce the testing space of remaining variables from all subsets to typical maximal cliques.</p> <p>Notice that the maximal cliques including the remaining variables whose causal orders were not determined are possibly formed by latent confounders. Correspondingly, the stage-II learning is continuously applied on the maximal cliques (instead of all subsets) to learn more causal orders if some variables are not affected by the latent confounders but are in the remaining subset, and ultimately evaluate whether these variables are share the same latent confounders.</p> <p>Parameters:</p> Name Type Description Default <code>graph_pattern_manager</code> <p>An auxiliary module embedded in the MLC-LiNGAM algorithm, featuring the algorithmic behavior of the maximal-cliques pattern recognition.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Update <code>_adjacency_matrix</code> as the estimated (partial) directed acyclic graph (DAG) corresponding to the causal graph, <code>_latent_confounder_detection</code> as the undirected maximal cliques after  stage-III learning, and <code>record _stage3_time</code> as the <code>stage-3</code>  computational time.</p> Source code in <code>cadimulc\\hybrid_algorithms\\hybrid_algorithms.py</code> <pre><code>def _stage_3_learning(self, graph_pattern_manager) -&gt; object:\n    \"\"\"\n    **Stage-III**: Latent confounders' detection\n\n    Stage-III will **detect** whether the remaining variables share the\n    **same latent confounders**.\n    Specifically, the stage-III learning makes use of the **causal skeleton** information\n    to reduce the **testing space** of remaining variables from all subsets to typical\n    **maximal cliques**.\n\n    Notice that the maximal cliques including the remaining variables\n    whose causal orders were not determined are possibly formed by latent confounders.\n    Correspondingly, the stage-II learning is continuously applied on the maximal cliques\n    (instead of all subsets) to learn more causal orders if some variables are not affected\n    by the latent confounders but are in the remaining subset, and ultimately\n    evaluate whether these variables are share the same latent confounders.\n\n    Parameters:\n        graph_pattern_manager:\n            An auxiliary module embedded in the MLC-LiNGAM algorithm,\n            featuring the algorithmic behavior of the maximal-cliques pattern recognition.\n\n    Returns:\n        self:\n            Update ``_adjacency_matrix`` as the estimated (partial) directed acyclic\n            graph (DAG) corresponding to the causal graph,\n            ``_latent_confounder_detection`` as the undirected maximal cliques after\n             stage-III learning, and `record _stage3_time` as the `stage-3`\n             computational time.\n    \"\"\"\n\n    # Arguments for testing:\n    #   _skeleton (attribute)\n    #   _adjacency_matrix (attribute)\n    #   _parents_set (attribute)\n    #   _dataset(attribute): ndarray\n\n    start = time.perf_counter()\n\n    # Recognize the maximal-clique pattern based on the causal skeleton.\n    maximal_cliques_completely_undetermined = (\n        GraphPatternManager.recognize_maximal_cliques_pattern(\n            causal_skeleton=self._skeleton,\n            adjacency_matrix=self._adjacency_matrix\n        )\n    )\n\n    # Setup of regression, referring to MLC-LiNGAM default settings.\n    regressor = LinearRegression()\n    residuals_dataset = cp.copy(self._dataset)\n\n    # Replace the variables in the clique with their corresponding residuals via\n    # regressing out the effect of their confounded parents that are outside the clique.\n    for maximal_clique in maximal_cliques_completely_undetermined:\n        # Record: Each of the variable requires a single replacement if necessary.\n        variables_replaced = {}\n        for variable in maximal_clique:\n            variables_replaced[variable] = set()\n\n        # Get undetermined pairs within a clique.\n        for i in maximal_clique:\n            for j in maximal_clique[maximal_clique.index(i) + 1:]:\n                parents_i = graph_pattern_manager.managing_parents_set[i]\n                parents_j = graph_pattern_manager.managing_parents_set[j]\n\n                # Conduct residuals replacement if the variables share the same parents.\n                if (parents_i &amp; parents_j) != set():\n                    confounded_parents = parents_i &amp; parents_j\n\n                    for confounder in confounded_parents:\n                        data_confounder = residuals_dataset[:, confounder]\n\n                        if confounder not in variables_replaced[i]:\n                            variables_replaced[i].add(confounder)\n\n                            data_i = residuals_dataset[:, i]\n                            residuals_i = get_residuals_scm(\n                                explanatory_data=data_confounder,\n                                explained_data=data_i,\n                                regressor=regressor\n                            )\n                            residuals_dataset[:, i] = residuals_i.squeeze()\n\n                        if confounder not in variables_replaced[j]:\n                            variables_replaced[j].add(confounder)\n\n                            data_j = residuals_dataset[:, j]\n                            residuals_j = get_residuals_scm(\n                                explanatory_data=data_confounder,\n                                explained_data=data_j,\n                                regressor=regressor\n                            )\n                            residuals_dataset[:, j] = residuals_j.squeeze()\n\n    # Apply Algorithm-2 on the maximal cliques.\n    for maximal_clique in maximal_cliques_completely_undetermined:\n        # Get adjacent set with respect to the variables within maximal cliques.\n        adjacent_set_clique = {}\n        for variable in maximal_clique:\n            adjacent_set_clique[variable] = set(maximal_clique) - {variable}\n\n        # Apply Algorithm-2 (given by the MLC-LiNGAM algorithm).\n        self._algorithm_2(\n            corresponding_adjacent_set=adjacent_set_clique,\n            corresponding_dataset=residuals_dataset,\n            corresponding_variables=np.array(maximal_clique),\n            graph_pattern_manager=graph_pattern_manager,\n            _specify_adjacency=True,\n            _adjacent_set=adjacent_set_clique\n        )\n\n    # Update latent confounder detection\n    graph_pattern_manager.store_last_managing_adjacency_matrix()\n    self._latent_confounder_detection = (\n        graph_pattern_manager.get_undetermined_cliques(\n            maximal_cliques=maximal_cliques_completely_undetermined\n        )\n    )\n\n    # Record computational time.\n    end = time.perf_counter()\n    self._stage3_time = end - start\n\n    return self\n</code></pre> Class: Nonlinear-MLC <sup>[2]</sup> Primary Method: fit <p>Fitting data via the Nonlinear-MLC causal discovery algorithm.</p> <p>The procedure comprises the causal skeleton learning in the initial stage, along with the causal identification procedure involving non-linear regression and independence tests for the subsequence. Following the well-known divide-and-conquer strategy, non-linear causal inference are conducted over the maximal cliques recognized from the estimated causal skeleton.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>ndarray</code> <p>The observational dataset shown as a matrix or table, with a format of \"sample (n) * dimension (d).\" (input as Pandas dataframe is also acceptable)</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Update the <code>adjacency_matrix</code> represented as an estimated causal graph. The <code>adjacency_matrix</code> is a (d * d) numpy array with 0/1 elements characterizing the causal direction.</p> Source code in <code>cadimulc\\hybrid_algorithms\\hybrid_algorithms.py</code> <pre><code>def fit(self, dataset: ndarray) -&gt; object:\n    \"\"\"\n    Fitting data via the *Nonlinear-MLC* causal discovery algorithm.\n\n    The procedure comprises the **causal skeleton learning** in the initial stage,\n    along with the causal identification procedure involving **non-linear regression**\n    and **independence tests** for the subsequence.\n    Following the well-known **divide-and-conquer** strategy, non-linear causal inference\n    are conducted over the maximal cliques recognized from the estimated causal skeleton.\n\n    Parameters:\n        dataset:\n            The observational dataset shown as a matrix or table,\n            with a format of \"sample (n) * dimension (d).\"\n            (input as Pandas dataframe is also acceptable)\n\n    Returns:\n        self:\n            Update the ``adjacency_matrix`` represented as an estimated causal graph.\n            The ``adjacency_matrix`` is a (d * d) numpy array with 0/1 elements\n            characterizing the causal direction.\n    \"\"\"\n\n    start = time.perf_counter()\n\n    # Reconstruct a causal skeleton using the PC-stable algorithm.\n    self._causal_skeleton_learning(dataset)\n\n    # Recognize the maximal-clique pattern based on the causal skeleton.\n    maximal_cliques = GraphPatternManager.recognize_maximal_cliques_pattern(\n        causal_skeleton=self._skeleton\n    )\n\n    # Initialize a graph pattern manager for subsequent learning.\n    graph_pattern_manager = GraphPatternManager(\n        init_graph=self._skeleton\n    )\n\n    # Perform the nonlinear-mlc causal discovery.\n    continue_search = True\n    while continue_search:\n\n        # Obtain the cliques that remain at least one edge undetermined.\n        undetermined_maximal_cliques = (\n            graph_pattern_manager.get_undetermined_cliques(maximal_cliques)\n        )\n\n        # End if all edges over the cliques have been determined.\n        if len(undetermined_maximal_cliques) == 0:\n            break\n\n        # Temporally store the adjacency matrix ahead of a search round.\n        graph_pattern_manager.store_last_managing_adjacency_matrix()\n\n        # In light of the L-ANMs theory (proposed in paper), start the search round\n        # by conducting non-linear causal inference based on maximal cliques.\n        determined_pairs = self._clique_based_causal_inference(\n            undetermined_maximal_cliques=undetermined_maximal_cliques\n        )\n\n        # Orient the determined causal directions\n        # after a search round over maximal cliques.\n        graph_pattern_manager.identify_directed_causal_pair(\n            determined_pairs=determined_pairs\n        )\n\n        # Update the causal adjacency matrix and parent-relations set\n        # after a search round over maximal cliques.\n        self._adjacency_matrix = (\n            graph_pattern_manager.managing_adjacency_matrix\n        )\n        self._parents_set = (\n            graph_pattern_manager.managing_parents_set\n        )\n\n        # Check if new causal relations have been determined\n        # after the last round searching.\n        newly_determined = (\n            graph_pattern_manager.check_newly_determined(\n                undetermined_maximal_cliques\n            )\n        )\n\n        # End if there is none of new causal relation advancing the further search.\n        if not newly_determined:\n            continue_search = False\n\n    end = time.perf_counter()\n\n    self._running_time += (end - start)\n\n    return self\n</code></pre> Private Method: _clique_based_causal_inference <p>For each of the undetermined maximal cliques (e.g. at least one edge within a maximal clique remains undirected) with respect to the whole maximal-clique patterns, the algorithm conducts non-linear regression and independence tests with the additional explanatory variables selected from the undetermined maximal clique.</p> <p>This strategy is argued to enhance the efficiency and robustness as to the non-linear causal discovery with multiple latent confounders, serving as the essence of the Nonlinear-MLC algorithm (See the \"Latent-ANMs Lemma\" in the relevant paper, Section 3).</p> <p>Parameters:</p> Name Type Description Default <code>undetermined_maximal_cliques</code> <code>list[list]</code> <p>A list of undetermined maximal cliques in which the element (maximal clique) involves at least one edge remaining undirected (e.g. <code>[[X, Y, Z]]</code> stands for the only one maximal clique  in the list). required <p>Returns:</p> Type Description <code>list</code> <p>The list of determined pairs over the inputting undetermined cliques after searching. (e.g. <code>[[X, Y], [Y, Z]]</code> stands for two of the determined pairs \"X -&gt; Y\" and \"Y -&gt; Z\").</p> Source code in <code>cadimulc\\hybrid_algorithms\\hybrid_algorithms.py</code> <pre><code>def _clique_based_causal_inference(\n        self,\n        undetermined_maximal_cliques: list[list]\n) -&gt; list:\n    \"\"\"\n    For each of the undetermined maximal cliques (e.g. at least one edge\n    within a maximal clique remains undirected) with respect to the whole\n    maximal-clique patterns,\n    the algorithm conducts non-linear regression and independence tests with\n    the **additional explanatory variables** selected from the\n    undetermined maximal clique.\n\n    This strategy is argued to enhance the **efficiency** and **robustness** as to the **non-linear causal\n    discovery with multiple latent confounders**, serving as the essence of the *Nonlinear-MLC* algorithm (See\n    the \"Latent-ANMs Lemma\" in the [relevant paper](https://xuanzhichen.github.io/work/papers/nonlinear_mlc.pdf),\n    Section 3).\n\n    Parameters:\n        undetermined_maximal_cliques:\n            A list of undetermined maximal cliques in which the element\n            (maximal clique) involves at least one edge remaining undirected\n            (e.g. `[[X, Y, Z]]` stands for the only one maximal clique\n            &lt;X, Y, Z&gt; in the list).\n\n    Returns:\n        The list of determined pairs over the inputting undetermined cliques\n         after searching.\n         (e.g. `[[X, Y], [Y, Z]]` stands for two of the determined pairs\n         \"X -&gt; Y\" and \"Y -&gt; Z\").\n    \"\"\"\n\n    # Arguments for Testing:\n    #     _adjacency_matrix(attribute): ndarray\n    #     _dataset(attribute): ndarray\n\n    determined_pairs = []\n\n    # Conduct non-linear causal inference based on each maximal clique unit.\n    for undetermined_maximal_clique in undetermined_maximal_cliques:\n\n        # Initialize the lists with elements of undetermined causal relations.\n        # e.g. the element (cause, effect) specifies \"cause -&gt; effect\"\n        undetermined_pairs = []\n\n        # Get undetermined pairs within a clique.\n        for i in undetermined_maximal_clique:\n            for j in undetermined_maximal_clique[\n                undetermined_maximal_clique.index(i) + 1:\n            ]:\n                if (self._adjacency_matrix[i][j] == 1) and (\n                    self._adjacency_matrix[j][i] == 1\n                ):\n                    undetermined_pairs.append([i, j])\n\n        # Conduct pairwise non-linear regression and independence tests.\n        for pair in undetermined_pairs:\n            determined = False\n\n            p_value_max = self.pc_alpha\n            causation = copy_and_rename(pair)\n\n            # Unravel the pairwise inferred directions respectively.\n            pair_temp = cp.copy(pair)\n            pair_temp.reverse()\n            pair_reversed = copy_and_rename(pair_temp)\n\n            for cause, effect in zip(pair, pair_reversed):\n\n                # ========== Empirical Regressor Construction ==========\n\n                # initialization of explanatory-and-explained variables\n                explanatory_vars = set()\n                explained_var = set()\n\n                # basic explanatory-and-explained variables: cause-effect\n                explanatory_vars.add(cause)\n                explained_var.add(effect)  # namely the effect variable\n\n                # Add explanatory variables to strengthen empirical regression:\n\n                # determined parent-relations amidst the algorithm memory\n                explanatory_vars = explanatory_vars | set(self._parents_set[effect])\n\n                # undetermined connections within the maximal clique\n                explanatory_vars = explanatory_vars | (\n                        set(undetermined_maximal_clique) - {effect}\n                )\n\n                # Regress the effect variable on empirical explanatory variables\n                # (in an attempt to cancel unobserved confounding).\n\n                explanatory_data = cp.copy(\n                    self._dataset[:, list(explanatory_vars)]\n                )\n\n                # namely the data with respect to the effect variable\n                explained_data = cp.copy(\n                    self._dataset[:, list(explained_var)]\n                )\n\n                # regressing residuals via fitting SCMs\n                residuals = get_residuals_scm(\n                    explanatory_data=explanatory_data,\n                    explained_data=explained_data,\n                    regressor=self.regressor\n                )\n\n                # Remove effects of parent-relations from the cause variable\n                # (in an attempt to cancel unobserved confounding).\n\n                cause_parents = list(self._parents_set[cause])\n\n                if len(cause_parents) &gt; 0:\n                    cause_data = get_residuals_scm(\n                        explanatory_data=self._dataset[:, cause_parents],\n                        explained_data=self._dataset[:, cause],\n                        regressor=self.regressor\n                    )\n                else:\n                    cause_data = cp.copy(self._dataset[:, cause])\n\n                # ================== Independence Test =================\n\n                # Conduct the independence test\n                # between the cause variable and regressing residuals.\n                p_value = conduct_ind_test(\n                    explanatory_data=cause_data,\n                    residuals=residuals,\n                    ind_test_method=self.ind_test\n                )\n\n                # One single inferred causal direction is determined given the\n                # maximal p-value exceeding the threshold of the significant level.\n                if p_value &gt; p_value_max:\n                    determined = True\n\n                    p_value_max = p_value\n                    causation = (cause, effect)\n\n            if determined:\n                determined_pairs.append(causation)\n\n        return determined_pairs\n</code></pre> Auxiliary Class: GraphPatternManager Primary Method: recognize_maximal_cliques_pattern <p>Recognize the maximal-cliques pattern based on a causal skeleton (undirected graph) or a partial causal adjacency matrix (partially directed graph).</p> <p>Reference by <code>recognize_maximal_cliques_pattern</code></p> <p>Bron, Coen, and Joep Kerbosch. \"Algorithm 457: finding all cliques of an undirected graph.\" Communications of the ACM 16, no. 9 (1973): 575-577. (Implementation by NetworkX)</p> <p>Parameters:</p> Name Type Description Default <code>causal_skeleton</code> <code>ndarray</code> <p>The undirected graph corresponding to the causal graph.</p> required <code>adjacency_matrix</code> <code>ndarray | None</code> <p>The (partially) directed acyclic graph (DAG) corresponding to the causal graph.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>maximal_cliques</code> <code>list[list]</code> <p>A whole list of \"maximal cliques\", along with each of the \"maximal clique\" element that is as well in form of list.</p> Source code in <code>cadimulc\\hybrid_algorithms\\hybrid_algorithms.py</code> <pre><code>@staticmethod\ndef recognize_maximal_cliques_pattern(\n        causal_skeleton: ndarray,\n        adjacency_matrix: ndarray | None = None\n) -&gt; list[list]:\n    \"\"\"\n    Recognize the maximal-cliques pattern based on a causal skeleton (undirected graph)\n    or a partial causal adjacency matrix (partially directed graph).\n\n    !!! note \"Reference by ``recognize_maximal_cliques_pattern``\"\n        Bron, Coen, and Joep Kerbosch.\n        \"Algorithm 457: finding all cliques of an undirected graph.\"\n        Communications of the ACM 16, no. 9 (1973): 575-577.\n        (Implementation by [NetworkX](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.clique.find_cliques.html))\n\n    Parameters:\n        causal_skeleton:\n            The undirected graph corresponding to the causal graph.\n        adjacency_matrix:\n            The (partially) directed acyclic graph (DAG) corresponding to the causal graph.\n\n    Returns:\n        maximal_cliques:\n            A whole list of \"maximal cliques\", along with each of the\n            \"maximal clique\" element that is as well in form of list.\n    \"\"\"\n    # Remove the edge that has been determined as to the skeleton\n    if adjacency_matrix is not None:\n        dim = adjacency_matrix.shape[0]\n        for i in range(dim):\n            for j in range(dim):\n                if (adjacency_matrix[i][j] == 1) and (adjacency_matrix[j][i] == 0):\n                    causal_skeleton[i][j] = 0\n                    causal_skeleton[j][i] = 0\n\n    # Search maximal cliques by the Bron-Kerbosch algorithm.\n    undirected_graph_nx = nx.from_numpy_array(causal_skeleton)\n    clique_iter = nx.find_cliques(undirected_graph_nx)\n    maximal_cliques_temp = [clique for clique in clique_iter]\n\n    # Remove the trivial graph from maximal_cliques.\n    maximal_cliques = []\n    for clique in maximal_cliques_temp:\n        if len(clique) &gt; 1:\n            maximal_cliques.append(clique)\n\n    return maximal_cliques\n</code></pre> Primary Method: find_adjacent_set <p>Given a causal skeleton (or a subset of the causal skeleton), find out the adjacent variables for each of the variable.</p> <p>Parameters:</p> Name Type Description Default <code>causal_skeleton</code> <code>ndarray</code> <p>The undirected graph corresponding to the causal graph.</p> required <p>Returns:</p> Name Type Description <code>adjacent_set</code> <code>dict</code> <p>A dictionary that describes the adjacency relations: <code>{variable: (adjacent variables)}</code>.</p> Source code in <code>cadimulc\\hybrid_algorithms\\hybrid_algorithms.py</code> <pre><code>@staticmethod\ndef find_adjacent_set(causal_skeleton: ndarray) -&gt; dict:\n    \"\"\"\n    Given a causal skeleton (or a subset of the causal skeleton),\n    find out the adjacent variables for each of the variable.\n\n    Parameters:\n        causal_skeleton:\n            The undirected graph corresponding to the causal graph.\n\n    Returns:\n        adjacent_set:\n            A dictionary that describes the adjacency relations:\n            `{variable: (adjacent variables)}`.\n    \"\"\"\n\n    dim = causal_skeleton.shape[1]\n    adjacent_set = {}\n\n    for i in range(dim):\n        adjacent_set[i] = set()\n\n    for i in range(dim):\n        for j in range(dim):\n            if i != j:\n                if causal_skeleton[i][j] == 1:\n                    adjacent_set[i].add(j)\n\n    return adjacent_set\n</code></pre> Base Class: HybridFrameworkBase Primary Method: _causal_skeleton_learning <p>Causal skeleton construction (based on the PC-stable algorithm).</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>ndarray</code> <p>The observational dataset shown as a matrix or table, with a format of \"sample (n) * dimension (d).\" (input as Pandas dataframe is also acceptable)</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Update <code>_skeleton</code> as the estimated undirected graph corresponding to the causal graph, initialize <code>_adjacency_matrix</code> via a copy of <code>_skeleton</code>, and record <code>_stage1_time</code> as the stage-1 computational time (causal skeleton learning is usually the first stage in hybrid-based causal discovery algorithm) .</p> Source code in <code>cadimulc\\hybrid_algorithms\\hybrid_framework.py</code> <pre><code>def _causal_skeleton_learning(self, dataset: ndarray) -&gt; object:\n    \"\"\"\n    Causal skeleton construction (based on the PC-stable algorithm).\n\n    Parameters:\n        dataset:\n            The observational dataset shown as a matrix or table,\n            with a format of \"sample (n) * dimension (d).\"\n            (input as Pandas dataframe is also acceptable)\n\n    Returns:\n        self:\n            Update `_skeleton` as the estimated undirected graph corresponding to\n            the causal graph, initialize `_adjacency_matrix` via a copy of `_skeleton`,\n            and record `_stage1_time` as the stage-1 computational time\n            (causal skeleton learning is usually the first stage in\n            hybrid-based causal discovery algorithm) .\n    \"\"\"\n\n    # Arguments for testing:\n    #   pc_alpha(parameter): float\n    #   _dataset(attribute): dataframe\n\n    self._dim = dataset.shape[1]\n    self._dataset = dataset\n\n    for i in range(self._dim):\n        self._parents_set[i] = set()\n\n    data = cp.copy(self._dataset)\n\n    # Development notes:\n    # Unify the linear independence test even for nonlinear-mlc.\n    skeleton, running_time = get_skeleton_from_pc(\n        data=data,\n        alpha=self.pc_alpha,\n        ind_test_type='linear'\n    )\n\n    self._skeleton = cp.copy(skeleton)\n    self._adjacency_matrix = cp.copy(skeleton)\n    self._stage1_time = running_time\n\n    return self\n</code></pre>"},{"location":"hybrid_algorithms/#cadimulc.hybrid_algorithms.hybrid_algorithms.MLCLiNGAM","title":"<code>cadimulc.hybrid_algorithms.hybrid_algorithms.MLCLiNGAM</code>","text":"<p>             Bases: <code>HybridFrameworkBase</code></p> <p>MLC-LiNGAM stands for a hybrid causal discovery method for the LiNGAM approach with multiple latent confounders. It serves as an enhancement of LiNGAM via combining the advantages of constraint-based and functional-based causal discovery methodology.</p> <p>The LiNGAM causal discovery approach</p> <p>LiNGAM, the linear non-Gaussian acyclic model, is known as one of the structural-identifiable SCMs.</p> <p>MLC-LiNGAM was proposed to alleviate the following issues:</p> <ul> <li>how to detect the latent confounders;</li> <li>how to uncover the causal relations among observed and latent variables.</li> </ul> <p>The above challenges are partially tackled by MLC-LiNGAM, with the help of the maximal clique patterns raised by the latent confounders.</p>"},{"location":"hybrid_algorithms/#cadimulc.hybrid_algorithms.hybrid_algorithms.MLCLiNGAM.__init__","title":"<code>__init__(pc_alpha=0.05, _latent_confounder_detection=[])</code>","text":"<p>Parameters:</p> Name Type Description Default <code>pc_alpha</code> <code>float</code> <p>Significance level of independence tests (p_value), which is required by the constraint-based methodology incorporated in the initial stage of the hybrid causal discovery framework.</p> <code>0.05</code>"},{"location":"hybrid_algorithms/#cadimulc.hybrid_algorithms.hybrid_algorithms.NonlinearMLC","title":"<code>cadimulc.hybrid_algorithms.hybrid_algorithms.NonlinearMLC</code>","text":"<p>             Bases: <code>HybridFrameworkBase</code></p> <p>The hybrid algorithm Nonlinear-MLC, incorporation of the constraint-based and functional-based causal discovery methodology, is developed for the general causal inference over non-linear data in presence of multiple unknown factors.</p> <p>A primary feature of Nonlinear-MLC lies in exploiting the non-linear causal identification with multiple latent confounders (proposed as the Latent-ANMs causal identification), which is on the basic of the well-known non-linear causal additive-noise-models (ANMs).</p>"},{"location":"hybrid_algorithms/#cadimulc.hybrid_algorithms.hybrid_algorithms.NonlinearMLC.__init__","title":"<code>__init__(regressor=LinearGAM(), ind_test='kernel_ic', pc_alpha=0.05)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>object</code> <p>Built-in regressor modules are recommended: <code>from pygam import LinearGAM</code> or <code>from sklearn.neural_network import MLPRegressor</code></p> <code>LinearGAM()</code> <code>ind_test</code> <code>str</code> <p>Popular independence tests methods are recommended: Kernel-based Conditional Independence tests (KCI); Hilbert-Schmidt Independence Criterion (HSIC) for General Additive Models (GAMs).</p> <code>'kernel_ic'</code> <code>pc_alpha</code> <code>float</code> <p>Significance level of independence tests (p_value), which is required by the constraint-based methodology incorporated in the initial stage of the hybrid causal discovery framework.</p> <code>0.05</code>"},{"location":"hybrid_algorithms/#cadimulc.hybrid_algorithms.hybrid_algorithms.GraphPatternManager","title":"<code>cadimulc.hybrid_algorithms.hybrid_algorithms.GraphPatternManager</code>","text":"<p>             Bases: <code>object</code></p> <p>An auxiliary module embedded in the MLC-LiNGAM and Nonlinear-MLC algorithms, featuring the algorithmic behavior of the maximal-cliques pattern recognition.</p> <p>The module as well manages adjacency matrices amidst the procedure between causal skeleton learning and causal direction orientation.</p>"},{"location":"hybrid_algorithms/#cadimulc.hybrid_algorithms.hybrid_framework.HybridFrameworkBase","title":"<code>cadimulc.hybrid_algorithms.hybrid_framework.HybridFrameworkBase</code>","text":"<p>A hybrid causal discovery framework with established implementations of discovering the causal skeleton by the Peter-Clark algorithm (PC algorithm). The framework is incorporated into the initial stage of both the Nonlinear-MLC and MLC-LiNGAM causal discovery algorithms.</p>"},{"location":"hybrid_algorithms/#cadimulc.hybrid_algorithms.hybrid_framework.HybridFrameworkBase.__init__","title":"<code>__init__(pc_alpha=0.5, _dataset=None, _dim=None, _skeleton=None, _adjacency_matrix=None, _parents_set={}, _running_time=0.0)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>pc_alpha</code> <code>float</code> <p>Significance level of independence tests (p_value), which is required by the constraint-based methodology incorporated in the initial stage of the hybrid causal discovery framework.</p> <code>0.5</code> <code>_dataset</code> <code>ndarray</code> <p>The observational dataset shown as a matrix or table, with a format of \"sample (n) * dimension (d).\" (input as Pandas dataframe is also acceptable)</p> <code>None</code> <code>_dim</code> <code>int</code> <p>int The variable dimension corresponding to the causal graph.</p> <code>None</code> <code>_skeleton</code> <code>ndarray</code> <p>The estimated undirected graph corresponding to the causal graph.</p> <code>None</code> <code>_adjacency_matrix</code> <code>ndarray</code> <p>The estimated directed acyclic graph (DAG) corresponding to the causal graph.</p> <code>None</code> <code>_parents_set</code> <code>dict</code> <p>dict The child-parents relations associating with the adjacency matrix.</p> <code>{}</code>"},{"location":"hybrid_algorithms/#reference","title":"Reference","text":"<p>[1] Chen, Wei, Ruichu Cai, Kun Zhang, and Zhifeng Hao. \"Causal discovery in linear non-gaussian acyclic model with multiple latent confounders. \" IEEE Transactions on Neural Networks and Learning Systems. 2021.</p> <p>[2] Chen, Xuanzhi, Wei Chen, Ruichu Cai.  \"Non-linear Causal Discovery for Additive Noise Model with     Multiple Latent Confounders\". Xuanzhi's Personal Website. 2023.</p> <p>[3] Shimizu, Shohei, Patrik O. Hoyer, Aapo Hyv\u00e4rinen, Antti Kerminen, and Michael Jordan.  \"A linear non-Gaussian acyclic model for causal discovery.\"  Journal of Machine Learning Research. 2006.</p> <p>[4] Hoyer, Patrik, Dominik Janzing, Joris M. Mooij, Jonas Peters, and Bernhard Sch\u00f6lkopf. \"Nonlinear causal discovery with additive noise models.\"  Advances in neural information processing systems. 2008.</p> <p>[5] Spirtes, Peter, Clark N. Glymour, and Richard Scheines. Causation, prediction, and search. MIT press, 2000.</p>"},{"location":"quick_tutorials/","title":"Quick Tutorials","text":"<p>Here is an exceptionally simple example to help take a look on the \"micro\" workflow of the  causal discovery task. By attaching to this mini-causal-discovery procedure,  you could lean to experience a complete causal discovery in minutes.</p> <p>Note</p> <p>This page is currently under building.</p>"}]}